{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LoRAX","text":"<p> Multi-LoRA inference server that scales to 1000s of fine-tuned LLMs </p> <p> </p>"},{"location":"#what-is-lorax","title":"\ud83d\udcd6 What is LoRAX?","text":"<p>LoRAX (LoRA eXchange) is a framework that allows users to serve thousands of fine-tuned models on a single GPU, dramatically reducing the cost of serving without compromising on throughput or latency.</p>"},{"location":"#features","title":"\ud83c\udf33 Features","text":"<ul> <li>\ud83d\ude85 Dynamic Adapter Loading: include any fine-tuned LoRA adapter from HuggingFace, Predibase, or any filesystem in your request, it will be loaded just-in-time without blocking concurrent requests. Merge adapters per request to instantly create powerful ensembles.</li> <li>\ud83c\udfcb\ufe0f\u200d\u2640\ufe0f Heterogeneous Continuous Batching: packs requests for different adapters together into the same batch, keeping latency and throughput nearly constant with the number of concurrent adapters.</li> <li>\ud83e\uddc1 Adapter Exchange Scheduling: asynchronously prefetches and offloads adapters between GPU and CPU memory, schedules request batching to optimize the aggregate throughput of the system.</li> <li>\ud83d\udc6c Optimized Inference: high throughput and low latency optimizations including tensor parallelism, pre-compiled CUDA kernels (flash-attention, paged attention, SGMV), quantization, token streaming.</li> <li>\ud83d\udea2 Ready for Production prebuilt Docker images, Helm charts for Kubernetes, Prometheus metrics, and distributed tracing with Open Telemetry. OpenAI compatible API supporting multi-turn chat conversations. Private adapters through per-request tenant isolation. Structured Output (JSON mode).</li> <li>\ud83e\udd2f Free for Commercial Use: Apache 2.0 License. Enough said \ud83d\ude0e.</li> </ul>"},{"location":"#models","title":"\ud83c\udfe0 Models","text":"<p>Serving a fine-tuned model with LoRAX consists of two components:</p> <ul> <li>Base Model: pretrained large model shared across all adapters.</li> <li>Adapter: task-specific adapter weights dynamically loaded per request.</li> </ul> <p>LoRAX supports a number of Large Language Models as the base model including Llama (including CodeLlama), Mistral (including Zephyr), and Qwen. See Supported Architectures for a complete list of supported base models.</p> <p>Base models can be loaded in fp16 or quantized with <code>bitsandbytes</code>, GPT-Q, or AWQ.</p> <p>Supported adapters include LoRA adapters trained using the PEFT and Ludwig libraries. Any of the linear layers in the model can be adapted via LoRA and loaded in LoRAX.</p>"},{"location":"#getting-started","title":"\ud83c\udfc3\u200d\u2642\ufe0f Getting Started","text":"<p>We recommend starting with our pre-built Docker image to avoid compiling custom CUDA kernels and other dependencies.</p>"},{"location":"#requirements","title":"Requirements","text":"<p>The minimum system requirements need to run LoRAX include:</p> <ul> <li>Nvidia GPU (Ampere generation or above)</li> <li>CUDA 11.8 compatible device drivers and above</li> <li>Linux OS</li> <li>Docker (for this guide)</li> </ul>"},{"location":"#launch-lorax-server","title":"Launch LoRAX Server","text":"<pre><code>model=mistralai/Mistral-7B-Instruct-v0.1\nvolume=$PWD/data\n\ndocker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data \\\n    ghcr.io/predibase/lorax:main --model-id $model\n</code></pre> <p>For a full tutorial including token streaming and the Python client, see Getting Started - Docker.</p>"},{"location":"#prompt-via-rest-api","title":"Prompt via REST API","text":"<p>Prompt base LLM:</p> <pre><code>curl 127.0.0.1:8080/generate \\\n    -X POST \\\n    -d '{\"inputs\": \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\", \"parameters\": {\"max_new_tokens\": 64}}' \\\n    -H 'Content-Type: application/json'\n</code></pre> <p>Prompt a LoRA adapter:</p> <pre><code>curl 127.0.0.1:8080/generate \\\n    -X POST \\\n    -d '{\"inputs\": \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\", \"parameters\": {\"max_new_tokens\": 64, \"adapter_id\": \"vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\"}}' \\\n    -H 'Content-Type: application/json'\n</code></pre> <p>See Reference - REST API for full details.</p>"},{"location":"#prompt-via-python-client","title":"Prompt via Python Client","text":"<p>Install:</p> <pre><code>pip install lorax-client\n</code></pre> <p>Run:</p> <pre><code>from lorax import Client\n\nclient = Client(\"http://127.0.0.1:8080\")\n\n# Prompt the base LLM\nprompt = \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\"\nprint(client.generate(prompt, max_new_tokens=64).generated_text)\n\n# Prompt a LoRA adapter\nadapter_id = \"vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\"\nprint(client.generate(prompt, max_new_tokens=64, adapter_id=adapter_id).generated_text)\n</code></pre> <p>See Reference - Python Client for full details.</p> <p>For other ways to run LoRAX, see Getting Started - Kubernetes, Getting Started - SkyPilot, and Getting Started - Local.</p>"},{"location":"#chat-via-openai-api","title":"Chat via OpenAI API","text":"<p>LoRAX supports multi-turn chat conversations combined with dynamic adapter loading through an OpenAI compatible API. Just specify any adapter as the <code>model</code> parameter.</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"EMPTY\",\n    base_url=\"http://127.0.0.1:8080/v1\",\n)\n\nresp = client.chat.completions.create(\n    model=\"alignment-handbook/zephyr-7b-dpo-lora\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n        },\n        {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n    ],\n    max_tokens=100,\n)\nprint(\"Response:\", resp.choices[0].message.content)\n</code></pre> <p>See OpenAI Compatible API for details.</p>"},{"location":"#acknowledgements","title":"\ud83d\ude47 Acknowledgements","text":"<p>LoRAX is built on top of HuggingFace's text-generation-inference, forked from v0.9.4 (Apache 2.0).</p> <p>We'd also like to acknowledge Punica for their work on the SGMV kernel, which is used to speed up multi-adapter inference under heavy load.</p>"},{"location":"#roadmap","title":"\ud83d\uddfa\ufe0f Roadmap","text":"<p>Our roadmap is tracked here.</p>"},{"location":"getting_started/docker/","title":"Docker","text":"<p>We recommend starting with our pre-built Docker image to avoid compiling custom CUDA kernels and other dependencies.</p>"},{"location":"getting_started/docker/#run-container-with-base-llm","title":"Run container with base LLM","text":"<p>In this example, we'll use Mistral-7B-Instruct as the base model, but you can use any supported model from HuggingFace.</p> <pre><code>model=mistralai/Mistral-7B-Instruct-v0.1\nvolume=$PWD/data  # share a volume with the container as a weight cache\n\ndocker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data \\\n    ghcr.io/predibase/lorax:main --model-id $model\n</code></pre> <p>Note</p> <p>The <code>main</code> tag will use the image built from the HEAD of the main branch of the repo. For the latest stable image (built from a  tagged version) use the <code>latest</code> tag.</p> <p>Note</p> <p>The LoRAX server in the pre-built Docker image is configured to listen on port 80 (instead of on the default port number, which is 3000).</p> <p>Note</p> <p>To use GPUs, you need to install the NVIDIA Container Toolkit. We also recommend using NVIDIA drivers with CUDA version 11.8 or higher.</p> <p>See the references docs for the Launcher to view all available options, or run the following from within your container:</p> <pre><code>lorax-launcher --help\n</code></pre>"},{"location":"getting_started/docker/#prompt-the-base-llm","title":"Prompt the base LLM","text":"RESTREST (Streaming)PythonPython (Streaming) <pre><code>curl 127.0.0.1:8080/generate \\\n    -X POST \\\n    -d '{\"inputs\": \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\", \"parameters\": {\"max_new_tokens\": 64}}' \\\n    -H 'Content-Type: application/json'\n</code></pre> <pre><code>curl 127.0.0.1:8080/generate_stream \\\n    -X POST \\\n    -d '{\"inputs\": \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\", \"parameters\": {\"max_new_tokens\": 64}}' \\\n    -H 'Content-Type: application/json'\n</code></pre> <pre><code>pip install lorax-client\n</code></pre> <pre><code>from lorax import Client\n\nclient = Client(\"http://127.0.0.1:8080\")\nprompt = \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\"\n\nprint(client.generate(prompt, max_new_tokens=64).generated_text)\n</code></pre> <pre><code>pip install lorax-client\n</code></pre> <pre><code>from lorax import Client\n\nclient = Client(\"http://127.0.0.1:8080\")\nprompt = \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\"\n\ntext = \"\"\nfor response in client.generate_stream(prompt, max_new_tokens=64):\n    if not response.token.special:\n        text += response.token.text\nprint(text)\n</code></pre>"},{"location":"getting_started/docker/#prompt-a-lora-adapter","title":"Prompt a LoRA adapter","text":"RESTREST (Streaming)PythonPython (Streaming) <pre><code>curl 127.0.0.1:8080/generate \\\n    -X POST \\\n    -d '{\"inputs\": \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\", \"parameters\": {\"max_new_tokens\": 64, \"adapter_id\": \"vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\"}}' \\\n    -H 'Content-Type: application/json'\n</code></pre> <pre><code>curl 127.0.0.1:8080/generate_stream \\\n    -X POST \\\n    -d '{\"inputs\": \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\", \"parameters\": {\"max_new_tokens\": 64, \"adapter_id\": \"vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\"}}' \\\n    -H 'Content-Type: application/json'\n</code></pre> <pre><code>pip install lorax-client\n</code></pre> <pre><code>from lorax import Client\n\nclient = Client(\"http://127.0.0.1:8080\")\nprompt = \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\"\nadapter_id = \"vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\"\n\nprint(client.generate(prompt, max_new_tokens=64, adapter_id=adapter_id).generated_text)\n</code></pre> <pre><code>pip install lorax-client\n</code></pre> <pre><code>from lorax import Client\n\nclient = Client(\"http://127.0.0.1:8080\")\nprompt = \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\"\nadapter_id = \"vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\"\n\ntext = \"\"\nfor response in client.generate_stream(prompt, max_new_tokens=64, adapter_id=adapter_id):\n    if not response.token.special:\n        text += response.token.text\nprint(text)\n</code></pre>"},{"location":"getting_started/kubernetes/","title":"Kubernetes (Helm)","text":"<p>LoRAX includes Helm charts that make it easy to start using LoRAX in production with high availability and load balancing on Kubernetes.</p> <p>To spin up a LoRAX deployment with Helm, you only need to be connected to a Kubernetes cluster through `kubectl``. We provide a default values.yaml file that can be used to deploy a Mistral 7B base model to your Kubernetes cluster:</p> <pre><code>helm install mistral-7b-release charts/lorax\n</code></pre> <p>The default values.yaml configuration deploys a single replica of the Mistral 7B model. You can tailor configuration parameters to deploy any Llama or Mistral model by creating a new values file from the template and updating variables. Once a new values file is created, you can run the following command to deploy your LLM with LoRAX:</p> <pre><code>helm install -f your-values-file.yaml your-model-release charts/lorax\n</code></pre> <p>To delete the resources:</p> <pre><code>helm uninstall your-model-release\n</code></pre>"},{"location":"getting_started/local/","title":"Local","text":"<p>Advanced users or contributors may opt to install LoRAX locally.</p> <p>First install Rust and create a Python virtual environment with at least Python 3.9, e.g. using <code>conda</code>:</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n\nconda create -n lorax python=3.9 \nconda activate lorax\n</code></pre> <p>You may also need to install Protoc.</p> <p>On Linux:</p> <pre><code>PROTOC_ZIP=protoc-21.12-linux-x86_64.zip\ncurl -OL https://github.com/protocolbuffers/protobuf/releases/download/v21.12/$PROTOC_ZIP\nsudo unzip -o $PROTOC_ZIP -d /usr/local bin/protoc\nsudo unzip -o $PROTOC_ZIP -d /usr/local 'include/*'\nrm -f $PROTOC_ZIP\n</code></pre> <p>On MacOS, using Homebrew:</p> <pre><code>brew install protobuf\n</code></pre> <p>Then run:</p> <pre><code>BUILD_EXTENSIONS=True make install # Install repository and HF/transformer fork with CUDA kernels\nmake run-mistral-7b-instruct\n</code></pre> <p>Note: on some machines, you may also need the OpenSSL libraries and gcc. On Linux machines, run:</p> <pre><code>sudo apt-get install libssl-dev gcc -y\n</code></pre>"},{"location":"getting_started/local/#cuda-kernels","title":"CUDA Kernels","text":"<p>The custom CUDA kernels are only tested on NVIDIA A100s. If you have any installation or runtime issues, you can remove  the kernels by using the <code>DISABLE_CUSTOM_KERNELS=True</code> environment variable.</p> <p>Be aware that the official Docker image has them enabled by default.</p>"},{"location":"getting_started/local/#run-mistral","title":"Run Mistral","text":"<pre><code>make run-mistral-7b-instruct\n</code></pre>"},{"location":"getting_started/skypilot/","title":"SkyPilot","text":"<p>SkyPilot is a framework for running AI workloads in the cloud of your choice (AWS, Azure, GCP, etc.). It abstracts away the complexity of finding available GPU resources across clouds / zones, syncing data between storage systems, and managing the excution of distributed workloads.</p>"},{"location":"getting_started/skypilot/#setup","title":"Setup","text":"<p>First install SkyPilot and check that your cloud credentials are properly set:</p> <pre><code>pip install skypilot\nsky check\n</code></pre>"},{"location":"getting_started/skypilot/#launch-a-deployment","title":"Launch a deployment","text":"<p>Create a YAML configuration file called <code>lorax.yaml</code>:</p> <pre><code>resources:\n  cloud: aws\n  accelerators: A10G:1\n  memory: 32+\n  ports: \n    - 8080\n\nenvs:\n  MODEL_ID: mistralai/Mistral-7B-Instruct-v0.1\n\nrun: |\n  docker run --gpus all --shm-size 1g -p 8080:80 -v ~/data:/data \\\n    ghcr.io/predibase/lorax:main \\\n    --model-id $MODEL_ID\n</code></pre> <p>In the above example, we're asking SkyPilot to provision an AWS instance with 1 Nvidia A10G GPU and at least 32GB of RAM. Once the node is provisioned, SkyPilot will launch the LoRAX server using our latest pre-built Docker image.</p> <p>Let's launch our LoRAX job:</p> <pre><code>sky launch -c lorax-cluster lorax.yaml\n</code></pre> <p>By default, this config will deploy Mistral-7B-Instruct, but this can be overridden by running <code>sky launch</code> with the argument <code>--env MODEL_ID=&lt;my_model&gt;</code>.</p> <p>Warn</p> <p>This config will launch the instance on a public IP. It's highly recommended to secure the instance within a private subnet. See the Advanced Configurations section of the SkyPilot docs for options to run within VPC and setup private IPs.</p>"},{"location":"getting_started/skypilot/#prompt-lorax","title":"Prompt LoRAX","text":"<p>In a separate window, obtain the IP address of the newly created instance:</p> <pre><code>sky status --ip lorax-cluster\n</code></pre> <p>Now we can prompt the LoRAX deployment as usual:</p> <pre><code>IP=$(sky status --ip lorax-cluster)\n\ncurl http://$IP:8080/generate \\\n    -X POST \\\n    -d '{\"inputs\": \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\", \"parameters\": {\"max_new_tokens\": 64, \"adapter_id\": \"vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\"}}' \\\n    -H 'Content-Type: application/json'\n</code></pre>"},{"location":"getting_started/skypilot/#stop-the-deployment","title":"Stop the deployment","text":"<p>Stopping the deployment will shut down the instance, but keep the storage volume:</p> <pre><code>sky stop lorax-cluster\n</code></pre> <p>Because we set <code>docker run ... -v ~/data:/data</code> in our config from before, this means any model weights or adapters we downloaded will be persisted the next time we run <code>sky launch</code>. The LoRAX Docker image will also be cached, meaning tags like <code>latest</code> won't be updated on restart unless you add <code>docker pull</code> to your <code>run</code> configuration.</p>"},{"location":"getting_started/skypilot/#delete-the-deployment","title":"Delete the deployment","text":"<p>To completely delete the deployment, including the storage volume:</p> <pre><code>sky down lorax-cluster\n</code></pre> <p>The next time you run <code>sky launch</code>, the deployment will be recreated from scratch.</p>"},{"location":"guides/cuda_graphs/","title":"CUDA Graph Compilation","text":"<p>LoRAX supports compiling the model into a static CUDA Graph to speedup inference by upwards of 2x. See Accelerating PyTorch with CUDA Graphs for more details on CUDA graphs and how they can reduce latency.</p>"},{"location":"guides/cuda_graphs/#usage","title":"Usage","text":"<p>To enable this (experimental) feature:</p> <pre><code>lorax-launcher ... --compile\n</code></pre>"},{"location":"guides/cuda_graphs/#when-should-i-use-this","title":"When should I use this?","text":"<p>CUDA graph compilation is a simple way to decrease latency for smaller LLMs (O(1b params)) that are compute bound rather than memory bound.</p> <p>There is a tradeoff to be aware of when using CUDA graphs, namely that it increases memory overhead by 3-10GB depending on model size. However, the observed decrease in latency can be as much as 50%, so if you don't need to run with very large batch sizes and are more latency constrained than throughput, this is a very compelling feature to enable.</p> <p>In practice, CUDA graphs are most useful in cases where there are excess GPU flops available, such as during decoding. As such, we do not use the compiled version of the model during prefill, only during the decoding steps. Which means in practice that the benefits of enabling compilation will be most pronounced when generating longer sequences (for which more time is spent during decoding).</p>"},{"location":"guides/cuda_graphs/#limitations","title":"Limitations","text":"<p>Current limitations:</p> <ul> <li>Batch size &lt; 256</li> <li>Context length (input + output) &lt; 8192</li> <li>LoRA rank &gt;= 8 and &lt;= 64</li> <li>Only one LoRA rank in the batch</li> <li>1 GPU (no sharding)</li> </ul> <p>If any of these conditions are not met, then LoRAX will fallback to using eager execution for the batch.</p>"},{"location":"guides/cuda_graphs/#benchmarks","title":"Benchmarks","text":"<p>gpt2-medium, 1x A100, time to generate 100 tokens:</p> <p>no adapter:</p> <ul> <li>baseline: 1.044 s</li> <li>cuda graph: 0.422 s</li> </ul> <p>1 adapter (rank 16):</p> <ul> <li>baseline: 1.503 s</li> <li>cuda graph: 0.583 s</li> </ul>"},{"location":"guides/merging_adapters/","title":"Merging Adapters","text":"<p>In LoRAX, multiple LoRA adapters can be merged together per request to create powerful multi-task ensembles using one of several different merge strategies.</p> <p>This is particularly useful when you want your LLM to be capable of handling multiple types of tasks based on the user's prompt without requiring them to specify the type of task they wish to perform.</p>"},{"location":"guides/merging_adapters/#background-model-merging","title":"Background: Model Merging","text":"<p>Model merging is a set of techniques popularized by frameworks like mergekit that allow taking multiple specialized fine-tuned models and combining their weights together to output a single model that can perform each of these tasks with a much smaller total footprint.</p> <p>A common use case could be to train specialized LoRA adapters for tasks like SQL generation, customer support email generation, and information extraction. Without model merging, the user submitting their query will need to know in advance which of these models to route their query to. With model merging, the user should be able to submit their query without prior knowledge of which backing adapter is best suited to respond to the query.</p> <p>In some cases the mixing of adapter specializations could even result in a better final response. For example, by mixing an adapter that understand math with an adapter that can provide detailed and intuitive explanations, the user could in theory get correct answers to math questions with detailed step-by-step reasoning to aide in the user's learning.</p>"},{"location":"guides/merging_adapters/#merge-strategies","title":"Merge Strategies","text":"<p>LoRAX provides a number of model merging methods taken from mergekit and PEFT.</p> <p>Options:</p> <ul> <li><code>linear</code> (default)</li> <li><code>ties</code></li> <li><code>dare_linear</code></li> <li><code>dare_ties</code></li> </ul>"},{"location":"guides/merging_adapters/#linear","title":"Linear","text":"<p>The default and most straightforward way to merge model adapters is to linearly combine each of the parameters as a weighted average. This idea was  explored in the context of merging fine-tuned models in Model Soups.</p> <p>Parameters:</p> <ul> <li><code>weights</code> (default: <code>[1, ..]</code>): relative weight of each of the adapters in the request.</li> </ul>"},{"location":"guides/merging_adapters/#ties","title":"TIES","text":"<p>TIES is based on the idea of Task Arithmetic, whereby the fine-tuned models  are merged after subtracting out the base model weights. LoRA and other adapters are already task-specific tensors,  so this approach is a natural fit when merging LoRAs.</p> <p>To resolve interference between adapters, the weights are sparsified and a sign-based consensus algorithms is used to determine the weighted average.</p> <p>One the strengths of this approach is its ability to scale well to large numbers of adapters and retain each of their strengths.</p> <p>Parameters:</p> <ul> <li><code>weights</code> (default: <code>[1, ..]</code>): relative weight of each of the adapters in the request.</li> <li><code>density</code> (required): fraction of weights in adapters to retain.</li> <li><code>majority_sign_method</code> (default: <code>total</code>): one of <code>{total, frequency}</code> used to obtain the magnitude of the sign for consensus.</li> </ul>"},{"location":"guides/merging_adapters/#dare-linear","title":"DARE (Linear)","text":"<p>DARE, like TIES, sparsifies adapter weights (task vectors) to reduce interference. Unlike TIES, however, DARE uses random pruning and rescaling in an attempt to better match performance of the independent adapters.</p> <p>Parameters:</p> <ul> <li><code>weights</code> (default: <code>[1, ..]</code>): relative weight of each of the adapters in the request.</li> <li><code>density</code> (required): fraction of weights in adapters to retain.</li> </ul>"},{"location":"guides/merging_adapters/#dare-ties","title":"DARE (TIES)","text":"<p>DARE method from above that also applies the sign consensus algorithm from TIES.</p> <p>Parameters:</p> <ul> <li><code>weights</code> (default: <code>[1, ..]</code>): relative weight of each of the adapters in the request.</li> <li><code>density</code> (required): fraction of weights in adapters to retain.</li> <li><code>majority_sign_method</code> (default: <code>total</code>): one of <code>{total, frequency}</code> used to obtain the magnitude of the sign for consensus.</li> </ul>"},{"location":"guides/merging_adapters/#example","title":"Example","text":"<p>This example is derived from the PEFT example for model merging.</p> <p>First deploy LoRAX using the base model <code>TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T</code>, then run the following using the LoRAX Python Client:</p> <pre><code>from lorax import Client, MergedAdapters\n\nclient = Client(endpoint_url)\n\n# tinyllama merge\nmerged_adapters = MergedAdapters(\n    ids=[\n        \"smangrul/tinyllama_lora_norobots\",\n        \"smangrul/tinyllama_lora_sql\",\n        \"smangrul/tinyllama_lora_adcopy\",\n    ],\n    weights=[2.0, 0.3, 0.7],\n    merge_strategy=\"ties\",\n    density=0.2,\n    majority_sign_method=\"total\",\n)\n\n# norobots\nprompt = \"\"\"&lt;s&gt;&lt;|im_start|&gt;user\nWrite an essay about Generative AI.&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant \\n\"\"\"\nresponse = client.generate(prompt, merged_adapters=merged_adapters)\nprint(response.generated_text)\n\n# adcopy\nprompt = \"\"\"&lt;s&gt;&lt;|im_start|&gt;system \nCreate a text ad given the following product and description.&lt;|im_end|&gt; \n&lt;|im_start|&gt;user \nProduct: Sony PS5 PlayStation Console\nDescription: The PS5\u2122 console unleashes new gaming possibilities that you never anticipated.&lt;|im_end|&gt; \n&lt;|im_start|&gt;assistant \\n\"\"\"\nresponse = client.generate(prompt, merged_adapters=merged_adapters)\nprint(response.generated_text)\n\n# sql\nprompt = \"\"\"&lt;s&gt; Table: 2-11365528-2\nColumns: ['Team', 'Head Coach', 'President', 'Home Ground', 'Location']\nNatural Query: Who is the Head Coach of the team whose President is Mario Volarevic?\nSQL Query:\"\"\"\nresponse = client.generate(prompt, merged_adapters=merged_adapters)\nprint(response.generated_text)\n</code></pre>"},{"location":"guides/quantization/","title":"Quantization","text":"<p>LoRAX supports loading the base model with quantization to reduce memory overhead, while loading adapters in full (fp32) or half precision (fp16, bf16), similar to the approach described in QLoRA.</p> <p>When using quantization, it is not necessary that the adapter was fine-tuned using the quantized version of the base model, but be aware that enabling quantization can have an effect on the response.</p>"},{"location":"guides/quantization/#bitsandbytes","title":"bitsandbytes","text":"<p><code>bitsandbytes</code> quantization can be applied to any base model saved in fp16 or bf16 format. It performs quantization just-in-time at runtime in a model and dataset agnostic manner. As such, it is more flexible but potentially less performant (both in terms of quality and latency) than other quantization options.</p> <p>There are three flavors of <code>bitsandbytes</code> quantization:</p> <ul> <li><code>bitsandbytes</code> (8-bit integer)</li> <li><code>bitsandbytes-fp4</code> (4-bit float)</li> <li><code>bitsandbytes-nf4</code> (4-bit normal float)</li> </ul> <p>Usage:</p> <pre><code>lorax-launcher --model-id mistralai/Mistral-7B-v0.1 --quantize bitsandbytes-nf4 ...\n</code></pre>"},{"location":"guides/quantization/#awq","title":"AWQ","text":"<p>AWQ is a static quantization method applied outside of LoRAX using a framework such as AutoAWQ. Compared with other quantization methods, AWQ is very fast and very closely matches the quality of the original model, despite using int4 quantization.</p> <p>AWQ supports 4-bit quantization.</p> <p>Usage:</p> <pre><code>lorax-launcher --model-id TheBloke/Mistral-7B-v0.1-AWQ --quantize awq ...\n</code></pre>"},{"location":"guides/quantization/#gpt-q","title":"GPT-Q","text":"<p>GPT-Q is a static quantization method, meaning that the quantization needs to be done outside of LoRAX and the weights persisted in order for it to be used with a base model. GPT-Q offers faster inference performance compared with <code>bitsandbytes</code> but is noticeably slower than AWQ.</p> <p>Apart from inference speed, the major difference between AWQ and GPT-Q is the way quantization bins are determined. While AWQ looks at the distribution of activations, GPT-Q looks at the distribution of weights.</p> <p>GPT-Q supports 8 and 4-bit quantization, which is determined during quantized model creation outside of LoRAX.</p> <p>Usage:</p> <pre><code>lorax-launcher --model-id TheBloke/Mistral-7B-v0.1-GPTQ --quantize gptq ...\n</code></pre>"},{"location":"guides/quantization/#eetq","title":"EETQ","text":"<p>EETQ is an efficient just-in-time int8 quantization method that boasts very fast inference speed when compared against bitsandbytes.</p> <p>EETQ supports 8-bit quantization.</p> <p>Usage:</p> <pre><code>lorax-launcher --model-id mistralai/Mistral-7B-v0.1 --quantize eetq ...\n</code></pre>"},{"location":"guides/quantization/#hqq","title":"HQQ","text":"<p>HQQ is a fast just-in-time quantization method. Empircally, it is faster to load than other just-in-time methods like bitsandbytes or EETQ, but results in some amount of degradation in performance.</p> <p>HQQ supports 4, 3, and 2-bit quantization, making it particularly well suited to low VRAM GPUs.</p> <ul> <li><code>hqq-4bit</code> (4-bit)</li> <li><code>hqq-3bit</code> (3-bit)</li> <li><code>hqq-2bit</code> (2-bit)</li> </ul> <p>Usage:</p> <pre><code>lorax-launcher --model-id mistralai/Mistral-7B-v0.1 --quantize hqq-2bit ...\n</code></pre>"},{"location":"guides/speculative_decoding/","title":"Speculative Decoding","text":"<p>Speculative decoding describes a set of the methods for speeding up next token generation for autoregressive language models by attempting to \"guess\" the next N tokens of the base model. These guesses can be generated in a number of different ways including:</p> <ul> <li>An addtional smaller \"draft\" model (e.g., Llama-70b and Llama-7b)</li> <li>An adapter that extends the sequence dimension of the logits (e.g., Medusa)</li> <li>A heuristic (e.g., looking for recurring sequences in the prompt)</li> </ul> <p>LoRAX implements some of these approaches, with a particular emphasis on supporting adapter-based methods like Medusa that can be applied per request for task-level speedups.</p>"},{"location":"guides/speculative_decoding/#process","title":"Process","text":"<p>Most all of the above speculative decoding methods consist of the same two phases: a \"draft\" phase that generates candidate tokens and a \"verification\" phase that accepts some subset of the candidates to add to the response.</p>"},{"location":"guides/speculative_decoding/#draft","title":"Draft","text":"<p>For methods other than assisted generation via a draft model, the draft step happens at the end the normal next token selection phase after generating the logits. Given the logits for the next token and all the tokens that have been processed previously (input or output) a number of speculative tokens are generated and added to the batch state for verification in the next inference step.</p>"},{"location":"guides/speculative_decoding/#verification","title":"Verification","text":"<p>Once the speculative logits have been generated, a separate verification step is performed whereby the most likely next <code>S</code> tokens are passed through the model again (as part of the normal decoding process) to check for correctness. If any prefix of the <code>S</code> tokens are deemed correct, then they can be appended to the response directly. The remaining incorrect speculative tokens are discarded.</p> <p>Note that this process adds some compute overhead to the normal decoding step. As such, it will only confer benefits when:</p> <ol> <li>The decoding step is memory bound (generally true for most LLMs on modern GPUs).</li> <li>The speculation process is able to consistently predict future tokens correctly.</li> </ol>"},{"location":"guides/speculative_decoding/#options","title":"Options","text":""},{"location":"guides/speculative_decoding/#medusa","title":"Medusa","text":"<p>See the Medusa guide for details on how this method works and how to use it.</p>"},{"location":"guides/speculative_decoding/#prompt-lookup-decoding","title":"Prompt Lookup Decoding","text":"<p>Prompt Lookup Decoding is a simple herustic method that uses string matching on the input + previously generated tokens to find candidate n-grams. This method is particularly useful if your generation task will reuse many similar phrases from the input (e.g., in  retrieval augmented generation where citing the input is important). If there is no need to repeat anything from the input, there will be no speedup and performance may decrease.</p>"},{"location":"guides/speculative_decoding/#usage","title":"Usage","text":"<p>Initialize LoRAX with the <code>--speculative-tokens</code> param. This controls the length of the sequence LoRAX will attempt to match against in the input and suggest as the continuation of the current token:</p> <pre><code>docker run --gpus all --shm-size 1g -p 8080:80 -v $PWD:/data \\\n    ghcr.io/predibase/lorax:main \\\n    --model-id mistralai/Mistral-7B-Instruct-v0.2 \\\n    --speculative-tokens 3\n</code></pre> <p>Increasing this value will yield greater speedups when there are long common sequences, but slow things down if there is little overlap.</p> <p>Note that this method is not compatible with Medusa adapters per request.</p>"},{"location":"guides/structured_output/","title":"Structured Output (JSON)","text":"<p>LoRAX can enforce that responses consist only of valid JSON and adhere to a provided JSON schema.</p>"},{"location":"guides/structured_output/#background-structured-generation","title":"Background: Structured Generation","text":"<p>LoRAX enforces adherence to a schema through a process known as structured generation (also called constrained decoding).  Unlike guess-and-check validation methods, structured generation manipulates the next token likelihoods (logits) to enforce adherence to a schema at the token level. During each forward pass of inference, LLMs produce a probability distribution over their vocabulary of tokens. The token  that is actually generated is selected by sampling from this distribution. </p> <p>Suppose you've tasked an LLM with generating some valid JSON, and so far the LLM has produced the text <code>{ \"name\"</code>. When  considering the next token to output, it's clear that tokens like <code>A</code> or <code>&lt;</code> will not result in valid JSON. structured generation prevents the LLM from selecting an invalid token by modifying the probability distribution and setting the likelihood of invalid tokens to <code>-infinity</code>. In this way, we can guarantee that, at each step, only tokens that will produce valid JSON can be selected.</p>"},{"location":"guides/structured_output/#caveats","title":"Caveats","text":"<ul> <li>Structured generation does not guarantee the quality of generated text, only its form. structured generation may force the LLM to output valid JSON, but it can't ensure that the content of the JSON is desirable or accurate.</li> <li>Even with structured generation enabled, LLM output may not be fully valid JSON if the number of <code>max_new_tokens</code> is too low,     as this could result in necessary tokens (e.g., a closing <code>}</code>) being cut off.</li> </ul>"},{"location":"guides/structured_output/#structured-generation-with-outlines","title":"Structured Generation with Outlines","text":"<p>Outlines is an open-source library supporting various ways of specifying and enforcing structured generation rules onto LLM outputs.</p> <p>LoRAX uses Outlines to support structured generation following a user-provided JSON schema. This JSON schema is converted into a regular expression, and then into a finite-state machine (FSM). For each token, LoRAX then determines the set of valid next tokens using this FSM and sets the likelihood of invalid tokens to <code>-infinity</code>.</p>"},{"location":"guides/structured_output/#example-python-client","title":"Example: Python client","text":"<p>This example follows the JSON-structured generation example in the Outlines quickstart.</p> <p>We assume that you have already deployed LoRAX using a suitable base model and installed the LoRAX Python Client. Alternatively, see below for an example of structured generation using an  OpenAI client.</p> <pre><code>import json\nfrom enum import Enum\nfrom lorax import Client\nfrom pydantic import BaseModel, constr\n\n\nclass Armor(str, Enum):\n    leather = \"leather\"\n    chainmail = \"chainmail\"\n    plate = \"plate\"\n\n\nclass Character(BaseModel):\n    name: constr(max_length=10)\n    age: int\n    armor: Armor\n    strength: int\n\n\nclient = Client(\"http://127.0.0.1:8080\")\n\n# Example 1: Using a schema\nprompt_with_schema = \"Generate a new character for my awesome game: name, age (between 1 and 99), armor and strength.\"\nresponse_with_schema = client.generate(prompt_with_schema, response_format={\n    \"type\": \"json_object\",\n    \"schema\": Character.model_json_schema(),\n})\n\nmy_character_with_schema = json.loads(response_with_schema.generated_text)\\\nprint(my_character_with_schema)\n# {\n#    \"name\": \"Thorin\",\n#    \"age\": 45,\n#    \"armor\": \"plate\",\n#    \"strength\": 90\n# }\n\n# Example 2: Without a schema (arbitrary JSON)\nprompt_without_schema = \"Generate a new character for my awesome game: name, age (between 1 and 99), armor and strength.\"\nresponse_without_schema = client.generate(prompt_without_schema, response_format={\n    \"type\": \"json_object\",  # No schema provided\n})\n\nmy_character_without_schema = json.loads(response_without_schema.generated_text)\nprint(my_character_without_schema)\n# {\n#    \"characterName\": \"Aragon\",\n#    \"age\": 38,\n#    \"armorType\": \"chainmail\",\n#    \"power\": 78\n# }\n</code></pre> <p>You can also specify the JSON schema directly rather than using Pydantic:</p> <pre><code>schema = {\n    \"$defs\": {\n        \"Armor\": {\n            \"enum\": [\"leather\", \"chainmail\", \"plate\"],\n            \"title\": \"Armor\",\n            \"type\": \"string\"\n        }\n    },\n    \"properties\": {\n        \"name\": {\"maxLength\": 10, \"title\": \"Name\", \"type\": \"string\"},\n        \"age\": {\"title\": \"Age\", \"type\": \"integer\"},\n        \"armor\": {\"$ref\": \"#/$defs/Armor\"},\n        \"strength\": {\"title\": \"Strength\", \"type\": \"integer\"}\n    },\n    \"required\": [\"name\", \"age\", \"armor\", \"strength\"],\n    \"title\": \"Character\",\n    \"type\": \"object\"\n}\n</code></pre>"},{"location":"guides/structured_output/#example-openai-compatible-api","title":"Example: OpenAI-compatible API","text":"<p>Structured generation of JSON following a schema is supported via the <code>response_format</code> parameter.</p> <p>Note</p> <p>Currently, <code>response_format</code> in OpenAI interface differs slightly from the LoRAX request interface. When calling the OpenAI-compatible API, you should format the request exactly as specified in the official documentation. For more details, refer to the OpenAI documentation here: https://platform.openai.com/docs/api-reference/chat/create#chat-create-response_format.</p>"},{"location":"guides/structured_output/#type-1-text-default","title":"Type 1: <code>text</code> (default)","text":"<ul> <li>This is the standard mode where the model generates plain text output.</li> <li>In this example, the model simply returns plain text output.</li> </ul> <pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"EMPTY\",\n    base_url=\"http://127.0.0.1:8080/v1\",\n)\n\nresp = client.chat.completions.create(\n    model=\"\",  # optional: specify an adapter ID here\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Describe a medieval fantasy character.\",\n        },\n    ],\n    max_tokens=100,\n    response_format={\n        \"type\": \"text\",  # Default response type, plain text output\n    },\n)\n\nprint(resp.choices[0].message.content)\n\n'''\nSir Alaric is a noble knight of the realm. At the age of 35, he dons a suit of shining plate armor, protecting his strong, muscular frame. His strength is unparalleled in the kingdom, allowing him to wield his massive greatsword with ease.\n'''\n</code></pre>"},{"location":"guides/structured_output/#type-2-json_object","title":"Type 2: <code>json_object</code>","text":"<ul> <li>This mode outputs arbitrary JSON objects, making it ideal for generating data in a flexible JSON format without enforcing any schema. It's similar to OpenAI\u2019s JSON mode.</li> <li>In this example, the model returns an arbitrary JSON object without enforcing a predefined schema.</li> </ul> <pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"EMPTY\",\n    base_url=\"http://127.0.0.1:8080/v1\",\n)\n\nresp = client.chat.completions.create(\n    model=\"\",  # optional: specify an adapter ID here\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Generate a new character for my game: name, age, armor type, and strength.\",\n        },\n    ],\n    max_tokens=100,\n    response_format={\n        \"type\": \"json_object\",  # Generate arbitrary JSON without a schema\n    },\n)\n\nmy_character = json.loads(resp.choices[0].message.content)\nprint(my_character)\n\n'''\n{\n    \"name\": \"Eldrin\",\n    \"age\": 27,\n    \"armor\": \"Dragonscale Armor\",\n    \"strength\": \"Fire Resistance\"\n}\n'''\n</code></pre>"},{"location":"guides/structured_output/#type-3-json_schema","title":"Type 3: <code>json_schema</code>","text":"<ul> <li>The model returns a structured JSON object that adheres to the predefined schema. This ensures that the JSON follows the format of the <code>Character</code> model provided earlier.</li> <li>In this example, the model generates structured JSON output that adheres to a predefined schema.</li> </ul> <pre><code>import json\nfrom enum import Enum\nfrom openai import OpenAI\nfrom pydantic import BaseModel, constr\n\n\nclass Armor(str, Enum):\n    leather = \"leather\"\n    chainmail = \"chainmail\"\n    plate = \"plate\"\n\n\nclass Character(BaseModel):\n    name: constr(max_length=10)\n    age: int\n    armor: Armor\n    strength: int\n\n\nclient = OpenAI(\n    api_key=\"EMPTY\",\n    base_url=\"http://127.0.0.1:8080/v1\",\n)\n\nresp = client.chat.completions.create(\n    model=\"\",  # optional: specify an adapter ID here\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Generate a new character for my game: name, age (between 1 and 99), armor, and strength.\",\n        },\n    ],\n    max_tokens=100,\n    response_format={\n        \"type\": \"json_schema\",  # Generate structured JSON output based on a schema\n        \"json_schema\": {\n            \"name\": \"Character\",  # Name of the schema\n            \"schema\": Character.model_json_schema(),  # The JSON schema generated by Pydantic\n        },\n    },\n)\n\nmy_character = json.loads(resp.choices[0].message.content)\nprint(my_character)\n\n'''\n{\n    \"name\": \"Thorin\",\n    \"age\": 45,\n    \"armor\": \"plate\",\n    \"strength\": 90\n}\n'''\n</code></pre>"},{"location":"guides/contributing/","title":"Contributing to LoRAX","text":""},{"location":"guides/contributing/#setting-up-your-development-environment","title":"Setting up your development environment","text":"<p>See Development Environment.</p>"},{"location":"guides/contributing/#updating-python-server-dependencies","title":"Updating Python server dependencies","text":"<p>LoRAX uses Poetry to manage dependencies.</p> <p>When modifying the dependencies of the LoRAX Python server, first modify the server pyproject.toml file directly making the desired changes.</p> <p>Next, from within the <code>server</code> directory, generate an updated <code>poetry.lock</code> file:</p> <pre><code>poetry lock --no-update \n</code></pre> <p>Then (still within the <code>server</code> directory) generate a new <code>requirements.txt</code> file:</p> <pre><code>make export-requirements\n</code></pre> <p>Never modify <code>requirements.txt</code> directly, as it may introduce dependency conflicts.</p>"},{"location":"guides/contributing/#profiling","title":"Profiling","text":"<p>LoRAX supports the PyTorch Profiler to measure performance of LoRAX.</p> <p>You can enable profiling when launching LoRAX by setting the <code>LORAX_PROFILER_DIR</code> environment variable to the directory you wish to output the Tensorboard traces to.</p> <p>Once initialized, LoRAX will begin recording traces for every request to the server. Because traces can get very large, we record only the first 10 prefill requests (plus any decode requests between them), then stop recording and write out the results. A summary will be printed to stdout when this occurs.</p> <p>Once you have your traces written to the profiler directory, you can visualize them in Tensorboard using the PyTorch Profiler Tensorboard Plugin.</p> <pre><code>pip install torch_tb_profiler\ntensorboard --logdir=$LORAX_PROFILER_DIR\n</code></pre>"},{"location":"guides/contributing/development_env/","title":"Development Environment","text":"<p>This guide will walk through how you can setup a LoRAX devlopment environment within Docker.</p>"},{"location":"guides/contributing/development_env/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker</li> <li>Nvidia GPU (Ampere or newer)</li> <li>CUDA 11.8 drivers or above</li> </ul>"},{"location":"guides/contributing/development_env/#launch-development-container","title":"Launch development container","text":"<p>Pull and run the latest LoRAX docker image, mounting the directory containing your local <code>lorax</code> repo as a volume within the container:</p> <pre><code># we will assume the lorax repo is found at ~/data/lorax\nvolume=~/data\n\ndocker pull ghcr.io/predibase/lorax:main\ndocker run \\\n    --cap-add=SYS_PTRACE \\\n    --gpus all --shm-size 1g \\\n    -v $volume:/data \\\n    -itd --entrypoint /bin/bash ghcr.io/predibase/lorax:main\n</code></pre> <p>Note</p> <p><code>SYS_PTRACE</code> is set so we can obtain stack traces from running processes for debugging.</p> <p>Next, find the name of the container using <code>docker ps</code> and then SSH in:</p> <pre><code>docker exec -it &lt;container_id&gt; /bin/bash\n</code></pre> <p>Using two additional terminal windows, repeat the SSH process into the container.</p> <p>We'll be working out of three different terminals during development, each serving a different purpose:</p> <ol> <li>Server window running the Python LoRAX server.</li> <li>Router window running the Rust LoRAX router.</li> <li>Client window for executing requests against the running LoRAX instance.</li> </ol>"},{"location":"guides/contributing/development_env/#server-window-setup","title":"Server window setup","text":"<p>Install development dependencies:</p> <pre><code>apt update &amp;&amp; DEBIAN_FRONTEND=noninteractive apt install pkg-config rsync tmux rust-gdb git -y &amp;&amp; \\\nPROTOC_ZIP=protoc-21.12-linux-x86_64.zip &amp;&amp; \\\n    curl -OL https://github.com/protocolbuffers/protobuf/releases/download/v21.12/$PROTOC_ZIP &amp;&amp; \\\n    unzip -o $PROTOC_ZIP -d /usr/local bin/protoc &amp;&amp; \\\n    unzip -o $PROTOC_ZIP -d /usr/local 'include/*' &amp;&amp; \\\n    rm -f $PROTOC_ZIP &amp;&amp; \\\nhash -r\n</code></pre> <p>Download weights from HF:</p> <pre><code>lorax-server download-weights mistralai/Mistral-7B-Instruct-v0.1\n</code></pre> <p>Create <code>tmux</code> session so we don't lose our state when we close our laptop:</p> <pre><code>tmux new -s server\n</code></pre> <p>From within the <code>tmux</code> session, move into the LoRAX <code>server</code> directory within the repo (assumed to be in <code>/data/lorax</code>) and install dependencies:</p> <pre><code>cd /data/lorax/server &amp;&amp; pip install -e .\nmake gen-server\n</code></pre> <p>Launch the server:</p> <pre><code>SAFETENSORS_FAST_GPU=1 python -m torch.distributed.run \\\n    --nproc_per_node=1 lorax_server/cli.py \\\n    serve mistralai/Mistral-7B-Instruct-v0.1\n</code></pre>"},{"location":"guides/contributing/development_env/#router-window-setup","title":"Router window setup","text":"<p>As we did for the server, let's create a <code>tmux</code> session so we don't lose our state when we close our laptop:</p> <pre><code>tmux new -s router\n</code></pre> <p>Now move into the <code>router</code> directory within the repo and install dependencies:</p> <pre><code>cd /data/lorax/router &amp;&amp; \\\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y &amp;&amp; \\\nexport PATH=$PATH:$HOME/.cargo/bin &amp;&amp; \\\ntouch ../proto/generate.proto\n</code></pre> <p>Launch the router:</p> <pre><code>RUST_BACKTRACE=1 cargo run -- --port 8080 --tokenizer-name mistralai/Mistral-7B-Instruct-v0.1\n</code></pre>"},{"location":"guides/contributing/development_env/#client-window-setup","title":"Client window setup","text":"<p>From the third window, install the Python client from source:</p> <pre><code>cd /data/lorax/clients/python\npip install -e .\n</code></pre> <p>Now you can send requests to our LoRAX instance from either REST or Python locally:</p> <pre><code>curl 127.0.0.1:8080/generate \\\n    -X POST \\\n    -d '{\n        \"inputs\": \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\",\n        \"parameters\": {\n            \"max_new_tokens\": 64\n        }\n    }' \\\n    -H 'Content-Type: application/json'\n</code></pre>"},{"location":"guides/contributing/development_env/#optional-compile-cuda-kernels","title":"(Optional) Compile CUDA kernels","text":"<p>If you need compile CUDA kernels (for example, when updating to a newer version of a CUDA kernel), you'll need to install a few additional dependencies in your container.</p> <p>First, it recommended to run the kernel building from within a separate <code>tmux</code> session:</p> <pre><code>tmux new -s builder\n</code></pre> <p>Next, install the toolchain for compile CUDA kernels:</p> <pre><code>export PATH=/opt/conda/bin:$PATH\napt-get update &amp;&amp; DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \\\n    ninja-build \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nconda update --force conda\n/opt/conda/bin/conda install -c \"nvidia\"  cuda==12.4 cudnn &amp;&amp; \\\n    /opt/conda/bin/conda clean -ya\n</code></pre> <p>You should now have everything you need to build and install LoRAX's CUDA kernels:</p> <pre><code>cd /data/lorax/server/punica_kernels\nrm -rf build &amp;&amp; python setup.py build\ncp build/lib.linux-x86_64-cpython-310/punica_kernels.cpython-310-x86_64-linux-gnu.so /opt/conda/lib/python3.10/site-packages/.\n</code></pre>"},{"location":"http_status_codes/http_status/","title":"HTTP Status Codes Reference","text":"<p>When using Lorax, you may sometimes receive confusing HTTP messages. Refer to this file for definitions of HTTP status codes.</p>"},{"location":"http_status_codes/http_status/#1xx-informational-responses","title":"1xx: Informational Responses","text":"<ul> <li>100 Continue: The initial part of the request was received, and the client can continue.</li> <li>101 Switching Protocols: The server is switching protocols as requested.</li> <li>102 Processing: The server is processing the request but has not completed it yet (WebDAV).</li> <li>103 Early Hints: Used to return some response headers before the final HTTP message.</li> </ul>"},{"location":"http_status_codes/http_status/#2xx-success","title":"2xx: Success","text":"<ul> <li>200 OK: The request was successful.</li> <li>201 Created: The request succeeded, and a resource was created.</li> <li>202 Accepted: The request has been accepted for processing, but not completed.</li> <li>203 Non-Authoritative Information: The returned meta-information is not from the origin server.</li> <li>204 No Content: The server successfully processed the request but returned no content.</li> <li>205 Reset Content: The client should reset the document view.</li> <li>206 Partial Content: The server is delivering only part of the resource due to a range header sent by the client.</li> <li>207 Multi-Status: Conveys multiple status codes for a single request (WebDAV).</li> <li>208 Already Reported: The members of a DAV binding have already been enumerated (WebDAV).</li> <li>226 IM Used: The server has fulfilled a GET request for the resource (HTTP Delta Encoding).</li> </ul>"},{"location":"http_status_codes/http_status/#3xx-redirection","title":"3xx: Redirection","text":"<ul> <li>300 Multiple Choices: Multiple options for the resource are available.</li> <li>301 Moved Permanently: The URL of the requested resource has been permanently changed.</li> <li>302 Found: The resource resides temporarily under a different URL.</li> <li>303 See Other: The response to the request can be found under another URL.</li> <li>304 Not Modified: Indicates that the cached version is still valid.</li> <li>305 Use Proxy: The requested resource must be accessed through a proxy (deprecated).</li> <li>306 (Unused): Previously used, but no longer in use.</li> <li>307 Temporary Redirect: The resource temporarily resides under a different URL, and the method used should not be changed.</li> <li>308 Permanent Redirect: The resource is now permanently located at another URL.</li> </ul>"},{"location":"http_status_codes/http_status/#4xx-client-errors","title":"4xx: Client Errors","text":"<ul> <li>400 Bad Request: The request cannot be fulfilled due to bad syntax.</li> <li>401 Unauthorized: Authentication is required and has failed or has not been provided.</li> <li>402 Payment Required: Reserved for future use.</li> <li>403 Forbidden: The request was valid, but the server is refusing action.</li> <li>404 Not Found: The requested resource could not be found.</li> <li>405 Method Not Allowed: The request method is not supported for the requested resource.</li> <li>406 Not Acceptable: The requested resource is capable of generating only unacceptable content.</li> <li>407 Proxy Authentication Required: The client must authenticate itself with the proxy.</li> <li>408 Request Timeout: The server timed out waiting for the request.</li> <li>409 Conflict: Indicates a request conflict with the current state of the server.</li> <li>410 Gone: The resource is no longer available and will not be available again.</li> <li>411 Length Required: The server refuses to accept the request without a valid <code>Content-Length</code> header.</li> <li>412 Precondition Failed: The server does not meet one of the preconditions set by the client.</li> <li>413 Payload Too Large: The request entity is larger than the server is willing or able to process.</li> <li>414 URI Too Long: The URI provided was too long for the server to process.</li> <li>415 Unsupported Media Type: The media format of the requested data is not supported by the server.</li> <li>416 Range Not Satisfiable: The range specified by the <code>Range</code> header field cannot be fulfilled.</li> <li>417 Expectation Failed: The server cannot meet the requirements of the <code>Expect</code> header field.</li> <li>418 I'm a Teapot: A playful response defined in RFC 2324 (April Fools').</li> <li>421 Misdirected Request: The request was directed to a server that is unable to produce a response.</li> <li>422 Unprocessable Entity: The request was well-formed but unable to be followed due to semantic errors (WebDAV).</li> <li>423 Locked: The resource being accessed is locked (WebDAV).</li> <li>424 Failed Dependency: The request failed due to failure of a previous request (WebDAV).</li> <li>425 Too Early: The server is unwilling to risk processing a request that might be replayed.</li> <li>426 Upgrade Required: The client should switch to a different protocol.</li> <li>428 Precondition Required: The origin server requires the request to be conditional.</li> <li>429 Too Many Requests: The user has sent too many requests in a given amount of time.</li> <li>431 Request Header Fields Too Large: The server refuses to process the request due to large headers.</li> <li>451 Unavailable For Legal Reasons: The resource is unavailable due to legal reasons.</li> </ul>"},{"location":"http_status_codes/http_status/#5xx-server-errors","title":"5xx: Server Errors","text":"<ul> <li>500 Internal Server Error: A generic error occurred on the server.</li> <li>501 Not Implemented: The server does not recognize the request method.</li> <li>502 Bad Gateway: The server received an invalid response from the upstream server.</li> <li>503 Service Unavailable: The server is not ready to handle the request.</li> <li>504 Gateway Timeout: The upstream server failed to send a request in time.</li> <li>505 HTTP Version Not Supported: The server does not support the HTTP protocol version.</li> <li>506 Variant Also Negotiates: The server has an internal configuration error.</li> <li>507 Insufficient Storage: The server is unable to store the representation (WebDAV).</li> <li>508 Loop Detected: The server detected an infinite loop while processing the request (WebDAV).</li> <li>510 Not Extended: Further extensions to the request are required.</li> <li>511 Network Authentication Required: The client needs to authenticate to gain network access.</li> </ul>"},{"location":"models/base_models/","title":"Base Models","text":""},{"location":"models/base_models/#supported-architectures","title":"Supported Architectures","text":"<ul> <li>\ud83e\udd99 Llama</li> <li>CodeLlama</li> <li>\ud83c\udf2c\ufe0fMistral</li> <li>Zephyr</li> <li>\ud83d\udd04 Mixtral</li> <li>\ud83d\udc8e Gemma</li> <li>Gemma2</li> <li>\ud83c\udfdb\ufe0f Phi-3 / Phi-2</li> <li>\ud83d\udd2e Qwen2 / Qwen</li> <li>\ud83d\udde3\ufe0f Command-R</li> <li>\ud83e\uddf1 DBRX</li> <li>\ud83e\udd16 GPT2</li> <li>\ud83d\udd06 Solar</li> <li>\ud83c\udf38 Bloom</li> </ul> <p>Other architectures are supported on a best effort basis, but do not support dynamic adapter loading.</p>"},{"location":"models/base_models/#selecting-a-base-model","title":"Selecting a Base Model","text":"<p>Check the HuggingFace Hub to find supported base models.</p> <p>Usage:</p> <pre><code>lorax-launcher --model-id mistralai/Mistral-7B-v0.1 ...\n</code></pre>"},{"location":"models/base_models/#private-models","title":"Private Models","text":"<p>You can access private base models from HuggingFace by setting the <code>HUGGING_FACE_HUB_TOKEN</code> environment variable:</p> <pre><code>export HUGGING_FACE_HUB_TOKEN=&lt;YOUR READ TOKEN&gt;\n</code></pre> <p>Using Docker:</p> <pre><code>docker run --gpus all \\\n  --shm-size 1g \\\n  -p 8080:80 \\\n  -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \\\n  ghcr.io/predibase/lorax:main \\\n  --model-id $MODEL_ID\n</code></pre>"},{"location":"models/base_models/#quantization","title":"Quantization","text":"<p>LoRAX supports loading the base model with quantization to reduce memory overhead, while loading adapters in full (fp32) or half precision (fp16, bf16), similar to the approach described in QLoRA.</p> <p>See Quantization for details on the various quantization strategies provided by LoRAX.</p>"},{"location":"models/adapters/","title":"Adapters","text":"<p>Adapters are small model fragments that can be loaded on top of base models in LoRAX -- either during server initialization or at runtime as part of the request parameters.</p>"},{"location":"models/adapters/#types","title":"Types","text":""},{"location":"models/adapters/#lora","title":"LoRA","text":"<p>LoRA is a popular parameter efficient fine-tuning method to improve response quality.</p> <p>LoRAX can load any LoRA adapter dynamically at runtime per request, and batch many different LoRAs together at once for high throughput.</p> <p>Usage:</p> <pre><code>\"parameters\": {\n    \"adapter_id\": \"predibase/conllpp\"\n}\n</code></pre>"},{"location":"models/adapters/#medusa","title":"Medusa","text":"<p>Medusa is a speculative decoding method that speeds up next-token generation by attempting to generate more than one token at a time.</p> <p>LoRAX can load Medusa adapters dynamically at runtime per request provided that the LoRAX server was initialized with a default Medusa adapter.</p> <p>Usage:</p> <pre><code>\"parameters\": {\n    \"adapter_id\": \"predibase/Mistral-7B-Instruct-v0.2-magicoder-medusa\"\n}\n</code></pre>"},{"location":"models/adapters/#source","title":"Source","text":"<p>You can provide an adapter from the HuggingFace Hub, a local file path, or S3. </p> <p>Just make sure that the adapter was trained on the same base model used in the deployment. LoRAX only supports one base model at a time, but any number of adapters derived from it!</p>"},{"location":"models/adapters/#huggingface-hub","title":"Huggingface Hub","text":"<p>By default, LoRAX will load adapters from the Huggingface Hub.</p> <p>Usage:</p> <pre><code>\"parameters\": {\n    \"adapter_id\": \"vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\",\n    \"adapter_source\": \"hub\"\n}\n</code></pre>"},{"location":"models/adapters/#predibase","title":"Predibase","text":"<p>Any adapter hosted in Predibase can be used in LoRAX by setting <code>adapter_source=\"pbase\"</code>.</p> <p>When using Predibase hosted adapters, the <code>adapter_id</code> format is <code>&lt;model_repo&gt;/&lt;model_version&gt;</code>. If the <code>model_version</code> is omitted, the latest version in the Model Repoistory will be used.</p> <p>Usage:</p> <pre><code>\"parameters\": {\n    \"adapter_id\": \"model_repo/model_version\",\n    \"adapter_source\": \"pbase\"\n}\n</code></pre>"},{"location":"models/adapters/#local","title":"Local","text":"<p>When specifying an adapter in a local path, the <code>adapter_id</code> should correspond to the root directory of the adapter containing the following files:</p> <pre><code>root_adapter_path/\n    adapter_config.json\n    adapter_model.bin\n    adapter_model.safetensors\n</code></pre> <p>The weights must be in one of either a <code>adapter_model.bin</code> (pickle) or <code>adapter_model.safetensors</code> (safetensors) format. If both are provided, safestensors will be used.</p> <p>See the PEFT library for detailed examples showing how to save adapters in this format.</p> <p>Usage:</p> <pre><code>\"parameters\": {\n    \"adapter_id\": \"/data/adapters/vineetsharma--qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\",\n    \"adapter_source\": \"local\"\n}\n</code></pre>"},{"location":"models/adapters/#s3","title":"S3","text":"<p>Similar to a local path, an S3 path can be provided. Just make sure you have the appropriate environment variables <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> set so you can authenticate to AWS.</p> <p>Usage:</p> <pre><code>\"parameters\": {\n    \"adapter_id\": \"s3://adapters_bucket/vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\",\n    \"adapter_source\": \"s3\"\n}\n</code></pre>"},{"location":"models/adapters/#merging-adapters","title":"Merging Adapters","text":"<p>Multiple adapters can be mixed / merged together per request to create powerful ensembles of different specialized adapters.</p> <p>This is particularly useful when you want your LLM to be capable of handling multiple types of tasks based on the user's prompt without requiring them to specify the type of task they wish to perform.</p> <p>See Merging Adapters for details.</p>"},{"location":"models/adapters/#private-adapter-repositories","title":"Private Adapter Repositories","text":"<p>For hosted adapter repositories like HuggingFace Hub and Predibase, you can perform inference using private adapters per request.</p> <p>Usage:</p> <pre><code>\"parameters\": {\n    \"adapter_id\": \"my-repo/private-adapter\",\n    \"api_token\": \"&lt;auth_token&gt;\"\n}\n</code></pre> <p>If you prefer not to include the token in the request body, you can include it in the \"Authorization\" header:</p> PythonREST <pre><code>headers = {\n    \"Authorization\": f\"Bearer {api_token}\"\n}\nclient = Client(\"http://127.0.0.1:8080\", headers=headers)\n</code></pre> <pre><code>curl http://127.0.0.1:8080/generate \\\n    -X POST \\\n    -d '{\n        \"inputs\": \"...\"\n    }' \\\n    -H 'Content-Type: application/json' \\\n    -H \"Authorization: Bearer ${API_TOKEN}\"\n</code></pre> <p>The authorization check is performed per-request in the background (prior to batching to prevent slowing down inference) every time, so even if the adapter is cachd locally or the authorization token has been invalidated, the check will be performed and handled appropriately.</p> <p>For details on generating API tokens, see:</p> <ul> <li>HuggingFace docs</li> <li>Predibase docs</li> </ul>"},{"location":"models/adapters/#fallback-to-global-huggingface-token","title":"Fallback to Global HuggingFace Token","text":"<p>In some cases, you may want LoRAX to use the global <code>HUGGING_FACE_HUB_TOKEN</code> set in the environment for all adapter requests by default. To enable this, set the following in your environment:</p> <pre><code>export LORAX_USE_GLOBAL_HF_TOKEN=1\n</code></pre> <p>It is suggested to only enable this if you are comfortable with external callers to your server being able to access all of the adapters in your private HF repo.</p>"},{"location":"models/adapters/lora/","title":"LoRA","text":"<p>Low Rank Adaptation (LoRA) is a popular adapter method for fine-tuning response quality. </p> <p>LoRAX supports LoRA adapters trained using frameworks like PEFT and Ludwig.</p>"},{"location":"models/adapters/lora/#how-it-works","title":"How it works","text":"<pre><code>graph BT\n  I{{X}} --&gt; W;\n  I --&gt; A[/LoRA A\\];\n  A --&gt; B[\\LoRA B/];\n  W --&gt; P((+));\n  B--&gt; P;\n  P --&gt; O{{Y}}</code></pre> <p>LoRA works by targeting specific layers of the base model and inserting a new low-rank pair of weights <code>LoRA A</code> and <code>LoRA B</code> alongside each base model param <code>W</code>. The input <code>X</code> is passed through both the original weights and the LoRA weights, and then the activations are summed together to produce the final layer output <code>Y</code>.</p>"},{"location":"models/adapters/lora/#usage","title":"Usage","text":""},{"location":"models/adapters/lora/#supported-target-modules","title":"Supported Target Modules","text":"<p>When training a LoRA adapter, you can specify which of these layers (or \"modules\") you wish to target for adaptation. Typically these are the projection layers in the attention blocks (<code>q</code> and <code>v</code>, sometimes <code>k</code> and <code>o</code> as well for LLaMA like models), but can usually be any linear layer.</p> <p>Here is a list of supported target modules for each architecture in LoRAX. Note that in cases where your adapter contains target modules that LoRAX does not support, LoRAX will ignore those layers and emit a warning on the backend.</p>"},{"location":"models/adapters/lora/#llama","title":"Llama","text":"<ul> <li><code>q_proj</code></li> <li><code>k_proj</code></li> <li><code>v_proj</code></li> <li><code>o_proj</code></li> <li><code>gate_proj</code></li> <li><code>up_proj</code></li> <li><code>down_proj</code></li> <li><code>lm_head</code></li> </ul>"},{"location":"models/adapters/lora/#mistral","title":"Mistral","text":"<ul> <li><code>q_proj</code></li> <li><code>k_proj</code></li> <li><code>v_proj</code></li> <li><code>o_proj</code></li> <li><code>gate_proj</code></li> <li><code>up_proj</code></li> <li><code>down_proj</code></li> <li><code>lm_head</code></li> </ul>"},{"location":"models/adapters/lora/#mixtral","title":"Mixtral","text":"<ul> <li><code>q_proj</code></li> <li><code>k_proj</code></li> <li><code>v_proj</code></li> <li><code>o_proj</code></li> <li><code>lm_head</code></li> </ul>"},{"location":"models/adapters/lora/#gemma","title":"Gemma","text":"<ul> <li><code>q_proj</code></li> <li><code>k_proj</code></li> <li><code>v_proj</code></li> <li><code>o_proj</code></li> <li><code>gate_proj</code></li> <li><code>up_proj</code></li> <li><code>down_proj</code></li> </ul>"},{"location":"models/adapters/lora/#phi-3","title":"Phi-3","text":"<ul> <li><code>qkv_proj</code></li> <li><code>o_proj</code></li> <li><code>gate_up_proj</code></li> <li><code>down_proj</code></li> <li><code>lm_head</code></li> </ul>"},{"location":"models/adapters/lora/#phi-2","title":"Phi-2","text":"<ul> <li><code>q_proj</code></li> <li><code>k_proj</code></li> <li><code>v_proj</code></li> <li><code>dense</code></li> <li><code>fc1</code></li> <li><code>fc2</code></li> <li><code>lm_head</code></li> </ul>"},{"location":"models/adapters/lora/#qwen2","title":"Qwen2","text":"<ul> <li><code>q_proj</code></li> <li><code>k_proj</code></li> <li><code>v_proj</code></li> <li><code>o_proj</code></li> <li><code>gate_proj</code></li> <li><code>up_proj</code></li> <li><code>down_proj</code></li> <li><code>lm_head</code></li> </ul>"},{"location":"models/adapters/lora/#qwen","title":"Qwen","text":"<ul> <li><code>c_attn</code></li> <li><code>c_proj</code></li> <li><code>w1</code></li> <li><code>w2</code></li> <li><code>lm_head</code></li> </ul>"},{"location":"models/adapters/lora/#command-r","title":"Command-R","text":"<ul> <li><code>q_proj</code></li> <li><code>k_proj</code></li> <li><code>v_proj</code></li> <li><code>o_proj</code></li> <li><code>gate_proj</code></li> <li><code>up_proj</code></li> <li><code>down_proj</code></li> <li><code>lm_head</code></li> </ul>"},{"location":"models/adapters/lora/#dbrx","title":"DBRX","text":"<ul> <li><code>Wqkv</code></li> <li><code>out_proj</code></li> <li><code>lm_head</code></li> </ul>"},{"location":"models/adapters/lora/#gpt2","title":"GPT2","text":"<ul> <li><code>c_attn</code></li> <li><code>c_proj</code></li> <li><code>c_fc</code></li> </ul>"},{"location":"models/adapters/lora/#bloom","title":"Bloom","text":"<ul> <li><code>query_key_value</code></li> <li><code>dense</code></li> <li><code>dense_h_to_4h</code></li> <li><code>dense_4h_to_h</code></li> <li><code>lm_head</code></li> </ul>"},{"location":"models/adapters/lora/#how-to-train","title":"How to train","text":"<p>LoRA is a very popular fine-tuning method for LLMs, and as such there are a number of options for creating them from your data, including the following (non-exhaustive) options.</p>"},{"location":"models/adapters/lora/#open-source","title":"Open Source","text":"<ul> <li>PEFT</li> <li>Ludwig</li> </ul>"},{"location":"models/adapters/lora/#commercial","title":"Commercial","text":"<ul> <li>Predibase</li> </ul>"},{"location":"models/adapters/medusa/","title":"Medusa","text":"<p>Medusa is a speculative decoding method that trains new projection layers (similar to LoRA layers) for the purpose of predicting future tokens and speedng up the text generation process.</p>"},{"location":"models/adapters/medusa/#how-it-works","title":"How it works","text":"<pre><code>graph BT\n  X{{H}} --&gt; S((Stack));\n  X --&gt; M1[Medusa 1];\n  X --&gt; M2[Medusa 2];\n  X --&gt; M3[Medusa 3];\n  M1 --&gt; S;\n  M2 --&gt; S;\n  M3 --&gt; S;\n  S --&gt; LM[LM Head]\n  LM --&gt; L{{Logits}}</code></pre> <p>The goal of Medusa is to speed up text generation. Unlike LoRA, Medusa does not aim to improve response quality, and in fact enabling Medusa will have no effect at all on the model output itself. Instead, Medusa works by adding additional projections (called \"medusa heads\") that the last hidden state <code>H</code> of the LLM is passed through that attempt to predict the next N tokens (rather than just the next 1 token).</p> <p>The result is that the output logit shape of the model at each decoding step is no longer <code>[B, 1, V]</code> for batch size <code>B</code> and vocabulary size <code>V</code>, but instead <code>[B, S, V]</code> where <code>S</code> is the number of Medusa speculative heads <code>N</code> plus <code>1</code> for the original model head.</p> <p>See the Speculative Decoding guide for more information on the verification step that follows.</p>"},{"location":"models/adapters/medusa/#change-in-v2","title":"Change in v2","text":"<p>The original implementation of Medusa trained separate LM head layers for each Medusa head. This introduced significant memory overhead that made dynamic loading of these adapters prohibitive. In v2, Medusa heads now reuse the base model LM head, reducing memory overhead by an order of magnitude.</p> <p>LoRAX supports both v1 and v2 Medusa adapters, but only allows dynamic loading for v2 adapters. To see which version your Medusa adapter is, check the <code>config.json</code> file for the <code>version</code> property. If not specified, the adapter is assumed to be v1.</p>"},{"location":"models/adapters/medusa/#usage","title":"Usage","text":""},{"location":"models/adapters/medusa/#initializing-lorax-with-medusa","title":"Initializing LoRAX with Medusa","text":"<p>In order to use Medusa speculative decoding in LoRAX, you must initialize the LoRAX server with a valid Medusa adapter as the \"default\" adapter. This means that by default every request will use the default Medusa adapter unless overriden by the request parameters.</p> <p>Example:</p> <pre><code>docker run --gpus all --shm-size 1g -p 8080:80 -v $PWD:/data \\\n    ghcr.io/predibase/lorax:main \\\n    --model-id mistralai/Mistral-7B-Instruct-v0.2 \\\n    --adapter-id predibase/Mistral-7B-Instruct-v0.2-medusa\n</code></pre>"},{"location":"models/adapters/medusa/#dynamic-medusa-per-request","title":"Dynamic Medusa per Request","text":"<p>When using a v2 Medusa adapter as default, you can also apply per-request Medusa adapters (that must also be v2) to specialize the speculative decoding to the particular task.</p> <p>For example, you might have a general-purpose Medusa adapter as the default that improves throughput for most prompts by ~50%. But if you know your incoming request is for code generation, you might want to apply a task-specific Medusa adapter trained on only code generation examples for a ~100% speedup:</p> PythonREST <pre><code>from lorax import Client\n\nclient = Client(\"http://127.0.0.1:8080\")\nprompt = \"[INST] Write a Python function that takes a list of strings as input and returns a new list containing only the strings that are palindromes. [/INST]\"\n\nresp = client.generate(\n    prompt,\n    adapter_id=\"predibase/Mistral-7B-Instruct-v0.2-magicoder-medusa\",\n)\nprint(resp.generated_text)\n</code></pre> <pre><code>curl 127.0.0.1:8080/generate \\\n    -X POST \\\n    -d '{\n        \"inputs\": \"[INST] Write a Python function that takes a list of strings as input and returns a new list containing only the strings that are palindromes. [/INST]\",\n        \"parameters\": {\n            \"adapter_id\": \"predibase/Mistral-7B-Instruct-v0.2-magicoder-medusa\"\n        }\n    }' \\\n    -H 'Content-Type: application/json'\n</code></pre> <p>The one caveat to using per request Medusa adapters is that adapters loaded per request must have the same number of Medusa heads as the default Medusa adapter. This is because for now the number of speculative tokens generated per step is a constant defined during initialization.</p>"},{"location":"models/adapters/medusa/#combining-with-lora","title":"Combining with LoRA","text":"<p>When LoRAX has been initialized with a default Medusa, you may continue to use it with dynamic LoRA loading as usual:</p> PythonREST <pre><code>from lorax import Client\n\nclient = Client(\"http://127.0.0.1:8080\")\nprompt = \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\"\n\nresp = client.generate(\n    prompt,\n    adapter_id=\"vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\",\n)\nprint(resp.generated_text)\n</code></pre> <pre><code>curl 127.0.0.1:8080/generate \\\n    -X POST \\\n    -d '{\n        \"inputs\": \"[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? [/INST]\",\n        \"parameters\": {\n            \"adapter_id\": \"vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\"\n        }\n    }' \\\n    -H 'Content-Type: application/json'\n</code></pre> <p>The default Medusa adapter will be applied to every LoRA in the batch. In the future, we also plan to support LoRAs that come with their own Medusa heads (Medusa 2).</p>"},{"location":"models/adapters/medusa/#how-to-train","title":"How to train","text":"<p>The official Medusa GitHub repo contains recipes for training a Medusa v2 adapter, including the self-distillation process. Broadly, the steps needed to create a Medusa adapter are:</p> <ol> <li>Prepare a dataset of example prompts in the ShareGPT or OpenAI conversation JSON format.</li> <li>Generate responses from the base model you wish to adapt (Medusa 1).</li> <li>Fine-tune Medusa heads using the prompt + response dataset.</li> </ol>"},{"location":"models/adapters/medusa/#example","title":"Example","text":"<p>Clone the repo (note: using a fork here that includes some fixes for more recent versions of <code>transformers</code>):</p> <pre><code>git clone https://github.com/tgaddair/Medusa.git\ncd Medusa\n</code></pre> <p>Install dependencies:</p> <pre><code>pip install -e \".[train]\"\npip install -U accelerate huggingface-hub\n</code></pre> <p>Download the dataset:</p> <pre><code>sudo apt-get install git-lfs\ngit lfs install\ngit clone https://huggingface.co/datasets/Aeala/ShareGPT_Vicuna_unfiltered\n</code></pre> <p>Launch a LoRAX server:</p> <pre><code>docker run --gpus all --shm-size 1g -p 8080:80 -v $PWD:/data \\\n    ghcr.io/predibase/lorax:main \\\n    --model-id mistralai/Mistral-7B-Instruct-v0.2 \\\n    --adapter-id predibase/Mistral-7B-Instruct-v0.2-medusa\n</code></pre> <p>Create the self-distillation dataset:</p> <pre><code>python create_data.py \\\n    --input-filename ShareGPT_Vicuna_unfiltered/ShareGPT_V4.3_unfiltered_cleaned_split.json \\\n    --output-filename sharegpt-mistral-7b-instruct-02.json\n</code></pre> <p>Train:</p> <pre><code>python medusa/train/train_legacy.py --model_name_or_path mistralai/Mistral-7B-Instruct-v0.2 \\\n    --data_path /data/sharegpt-mistral-7b-instruct-02.json \\\n    --bf16 True \\\n    --output_dir sharegpt_mistral_7b_it_v02 \\\n    --num_train_epochs 3 \\\n    --per_device_train_batch_size 4 \\\n    --per_device_eval_batch_size 4 \\\n    --gradient_accumulation_steps 32 \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"no\" \\\n    --learning_rate 1e-3 \\\n    --weight_decay 0.0 \\\n    --warmup_ratio 0.1 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1 \\\n    --tf32 True \\\n    --model_max_length 2048 \\\n    --lazy_preprocess True \\\n    --medusa_num_heads 3 \\\n    --medusa_num_layers 1\n</code></pre> <p>Prompt with LoRAX:</p> <pre><code>curl 127.0.0.1:8080/generate \\\n    -X POST \\\n    -d '{\n        \"inputs\": \"[INST] What is the photograph filter called where the only part of the image is greyscale? [/INST]\",\n        \"parameters\": {\n            \"max_new_tokens\": 64,\n            \"adapter_id\": \"/data/sharegpt_mistral_7b_it_v02\",\n            \"adapter_source\": \"local\"\n        }\n    }' \\\n    -H 'Content-Type: application/json'\n</code></pre> <p>Next you can upload to HF and use as a base Medusa adapter or runtime Medusa adapter.</p>"},{"location":"reference/launcher/","title":"LoRAX Launcher","text":"<pre><code>LoRAX Launcher\n\nUsage: lorax-launcher [OPTIONS]\n\nOptions:\n      --model-id &lt;MODEL_ID&gt;\n          The name of the model to load. Can be a MODEL_ID as listed on &lt;https://hf.co/models&gt; like `gpt2` or `mistralai/Mistral-7B-Instruct-v0.1`. Or it can be a local directory containing the necessary files as saved by `save_pretrained(...)` methods of transformers\n\n          [env: MODEL_ID=]\n          [default: mistralai/Mistral-7B-Instruct-v0.1]\n\n      --adapter-id &lt;ADAPTER_ID&gt;\n          The name of the adapter to load. Can be a MODEL_ID as listed on &lt;https://hf.co/models&gt; or it can be a local directory containing the necessary files as saved by `save_pretrained(...)` methods of transformers. Should be compatible with the model specified in `model_id`\n\n          [env: ADAPTER_ID=]\n          [default: ]\n\n      --source &lt;SOURCE&gt;\n          The source of the model to load. Can be `hub` or `s3`. `hub` will load the model from the huggingface hub. `s3` will load the model from the predibase S3 bucket\n\n          [env: SOURCE=]\n          [default: hub]\n\n      --adapter-source &lt;ADAPTER_SOURCE&gt;\n          The source of the model to load. Can be `hub` or `s3` or `pbase` `hub` will load the model from the huggingface hub. `s3` will load the model from the predibase S3 bucket. `pbase` will load an s3 model but resolve the metadata from a predibase server\n\n          [env: ADAPTER_SOURCE=]\n          [default: hub]\n\n      --adapter-memory-fraction &lt;ADAPTER_MEMORY_FRACTION&gt;\n          Reservation of memory set aside for loading adapters onto the GPU.\n          Increasing this value will reduce the size of the KV cache in exchange for allowing more\n          adapters to be loaded onto the GPU at once.\n          This value is NOT scaled relative to `cuda_memory_fraction`, but is expressed in absolute terms.\n          This will be set to `0` if `preloaded_adapter_ids` are provided.\n\n          [env: ADAPTER_MEMORY_FRACTION=]\n          [default: 0.1]\n\n      --revision &lt;REVISION&gt;\n          The actual revision of the model if you're referring to a model on the hub. You can use a specific commit id or a branch like `refs/pr/2`\n\n          [env: REVISION=]\n\n      --validation-workers &lt;VALIDATION_WORKERS&gt;\n          The number of tokenizer workers used for payload validation and truncation inside the router\n\n          [env: VALIDATION_WORKERS=]\n          [default: 2]\n\n      --sharded &lt;SHARDED&gt;\n          Whether to shard the model across multiple GPUs By default LoRAX will use all available GPUs to run the model. Setting it to `false` deactivates `num_shard`\n\n          [env: SHARDED=]\n          [possible values: true, false]\n\n      --num-shard &lt;NUM_SHARD&gt;\n          The number of shards to use if you don't want to use all GPUs on a given machine. You can use `CUDA_VISIBLE_DEVICES=0,1 lorax-launcher... --num_shard 2` and `CUDA_VISIBLE_DEVICES=2,3 lorax-launcher... --num_shard 2` to launch 2 copies with 2 shard each on a given machine with 4 GPUs for instance\n\n          [env: NUM_SHARD=]\n\n      --quantize &lt;QUANTIZE&gt;\n          Whether you want the model to be quantized. This will use `bitsandbytes` for quantization on the fly, or `gptq`\n\n          [env: QUANTIZE=]\n          [possible values: bitsandbytes, bitsandbytes-nf4, bitsandbytes-fp4, gptq, awq]\n\n      --compile\n          Whether you want to compile the model into a CUDA graph. This will speed up decoding but increase GPU memory usage\n\n          [env: COMPILE=]\n\n      --dtype &lt;DTYPE&gt;\n          The dtype to be forced upon the model. This option cannot be used with `--quantize`\n\n          [env: DTYPE=]\n          [possible values: float16, bfloat16]\n\n      --trust-remote-code\n          Whether you want to execute hub modelling code. Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision\n\n          [env: TRUST_REMOTE_CODE=]\n\n      --max-concurrent-requests &lt;MAX_CONCURRENT_REQUESTS&gt;\n          The maximum amount of concurrent requests for this particular deployment. Having a low limit will refuse clients requests instead of having them wait for too long and is usually good to handle backpressure correctly\n\n          [env: MAX_CONCURRENT_REQUESTS=]\n          [default: 1024]\n\n      --max-best-of &lt;MAX_BEST_OF&gt;\n          This is the maximum allowed value for clients to set `best_of`. Best of makes `n` generations at the same time, and return the best in terms of overall log probability over the entire generated sequence\n\n          [env: MAX_BEST_OF=]\n          [default: 2]\n\n      --max-stop-sequences &lt;MAX_STOP_SEQUENCES&gt;\n          This is the maximum allowed value for clients to set `stop_sequences`. Stop sequences are used to allow the model to stop on more than just the EOS token, and enable more complex \"prompting\" where users can preprompt the model in a specific way and define their \"own\" stop token aligned with their prompt\n\n          [env: MAX_STOP_SEQUENCES=]\n          [default: 10]\n\n      --max-input-length &lt;MAX_INPUT_LENGTH&gt;\n          This is the maximum allowed input length (expressed in number of tokens) for users. The larger this value, the longer prompt users can send which can impact the overall memory required to handle the load. Please note that some models have a finite range of sequence they can handle\n\n          [env: MAX_INPUT_LENGTH=]\n          [default: 1792]\n\n      --max-total-tokens &lt;MAX_TOTAL_TOKENS&gt;\n          This is the most important value to set as it defines the \"memory budget\" of running clients requests. Clients will send input sequences and ask to generate `max_new_tokens` on top. with a value of `1512` users can send either a prompt of `1000` and ask for `512` new tokens, or send a prompt of `1` and ask for `1511` max_new_tokens. The larger this value, the larger amount each request will be in your RAM and the less effective batching can be\n\n          [env: MAX_TOTAL_TOKENS=]\n          [default: 2048]\n\n      --waiting-served-ratio &lt;WAITING_SERVED_RATIO&gt;\n          This represents the ratio of waiting queries vs running queries where you want to start considering pausing the running queries to include the waiting ones into the same batch. `waiting_served_ratio=1.2` Means when 12 queries are waiting and there's only 10 queries left in the current batch we check if we can fit those 12 waiting queries into the batching strategy, and if yes, then batching happens delaying the 10 running queries by a `prefill` run.\n\n          This setting is only applied if there is room in the batch as defined by `max_batch_total_tokens`.\n\n          [env: WAITING_SERVED_RATIO=]\n          [default: 0.3]\n\n      --max-batch-prefill-tokens &lt;MAX_BATCH_PREFILL_TOKENS&gt;\n          Limits the number of tokens for the prefill operation. Since this operation take the most memory and is compute bound, it is interesting to limit the number of requests that can be sent\n\n          [env: MAX_BATCH_PREFILL_TOKENS=]\n          [default: 4096]\n\n      --max-batch-total-tokens &lt;MAX_BATCH_TOTAL_TOKENS&gt;\n          **IMPORTANT** This is one critical control to allow maximum usage of the available hardware.\n\n          This represents the total amount of potential tokens within a batch. When using padding (not recommended) this would be equivalent of `batch_size` * `max_total_tokens`.\n\n          However in the non-padded (flash attention) version this can be much finer.\n\n          For `max_batch_total_tokens=1000`, you could fit `10` queries of `total_tokens=100` or a single query of `1000` tokens.\n\n          Overall this number should be the largest possible amount that fits the remaining memory (after the model is loaded). Since the actual memory overhead depends on other parameters like if you're using quantization, flash attention or the model implementation, LoRAX cannot infer this number automatically.\n\n          [env: MAX_BATCH_TOTAL_TOKENS=]\n\n      --max-waiting-tokens &lt;MAX_WAITING_TOKENS&gt;\n          This setting defines how many tokens can be passed before forcing the waiting queries to be put on the batch (if the size of the batch allows for it). New queries require 1 `prefill` forward, which is different from `decode` and therefore you need to pause the running batch in order to run `prefill` to create the correct values for the waiting queries to be able to join the batch.\n\n          With a value too small, queries will always \"steal\" the compute to run `prefill` and running queries will be delayed by a lot.\n\n          With a value too big, waiting queries could wait for a very long time before being allowed a slot in the running batch. If your server is busy that means that requests that could run in ~2s on an empty server could end up running in ~20s because the query had to wait for 18s.\n\n          This number is expressed in number of tokens to make it a bit more \"model\" agnostic, but what should really matter is the overall latency for end users.\n\n          [env: MAX_WAITING_TOKENS=]\n          [default: 20]\n\n      --eager-prefill\n\n          Whether to prioritize running prefill before decode to increase batch size during decode (throughput) over\n          liveness in earlier requests (latency). This is set to true by default.\n\n          [default: true]\n\n      --max-active-adapters &lt;MAX_ACTIVE_ADAPTERS&gt;\n          Maximum number of adapters that can be placed on the GPU and accept requests at a time\n\n          [env: MAX_ACTIVE_ADAPTERS=]\n          [default: 1024]\n\n      --adapter-cycle-time-s &lt;ADAPTER_CYCLE_TIME_S&gt;\n          The time in seconds between adapter exchanges\n\n          [env: ADAPTER_CYCLE_TIME_S=]\n          [default: 2]\n\n      --hostname &lt;HOSTNAME&gt;\n          The IP address to listen on\n\n          [env: HOSTNAME=]\n          [default: 0.0.0.0]\n\n  -p, --port &lt;PORT&gt;\n          The port to listen on\n\n          [env: PORT=]\n          [default: 3000]\n\n      --shard-uds-path &lt;SHARD_UDS_PATH&gt;\n          The name of the socket for gRPC communication between the webserver and the shards\n\n          [env: SHARD_UDS_PATH=]\n          [default: /tmp/lorax-server]\n\n      --master-addr &lt;MASTER_ADDR&gt;\n          The address the master shard will listen on. (setting used by torch distributed)\n\n          [env: MASTER_ADDR=]\n          [default: localhost]\n\n      --master-port &lt;MASTER_PORT&gt;\n          The address the master port will listen on. (setting used by torch distributed)\n\n          [env: MASTER_PORT=]\n          [default: 29500]\n\n      --huggingface-hub-cache &lt;HUGGINGFACE_HUB_CACHE&gt;\n          The location of the huggingface hub cache. Used to override the location if you want to provide a mounted disk for instance\n\n          [env: HUGGINGFACE_HUB_CACHE=]\n\n      --weights-cache-override &lt;WEIGHTS_CACHE_OVERRIDE&gt;\n          The location of the huggingface hub cache. Used to override the location if you want to provide a mounted disk for instance\n\n          [env: WEIGHTS_CACHE_OVERRIDE=]\n\n      --disable-custom-kernels\n          For some models (like llama), LoRAX implemented custom cuda kernels to speed up inference. Those kernels were only tested on A100. Use this flag to disable them if you're running on different hardware and encounter issues\n\n          [env: DISABLE_CUSTOM_KERNELS=]\n\n      --cuda-memory-fraction &lt;CUDA_MEMORY_FRACTION&gt;\n          Limit the CUDA available memory. The allowed value equals the total visible memory multiplied by cuda-memory-fraction\n\n          [env: CUDA_MEMORY_FRACTION=]\n          [default: 1.0]\n\n      --json-output\n          Outputs the logs in JSON format (useful for telemetry)\n\n          [env: JSON_OUTPUT=]\n\n      --otlp-endpoint &lt;OTLP_ENDPOINT&gt;\n          [env: OTLP_ENDPOINT=]\n\n      --cors-allow-origin &lt;CORS_ALLOW_ORIGIN&gt;\n          [env: CORS_ALLOW_ORIGIN=]\n\n      --watermark-gamma &lt;WATERMARK_GAMMA&gt;\n          [env: WATERMARK_GAMMA=]\n\n      --watermark-delta &lt;WATERMARK_DELTA&gt;\n          [env: WATERMARK_DELTA=]\n\n      --ngrok\n          Enable ngrok tunneling\n\n          [env: NGROK=]\n\n      --ngrok-authtoken &lt;NGROK_AUTHTOKEN&gt;\n          ngrok authentication token\n\n          [env: NGROK_AUTHTOKEN=]\n\n      --ngrok-edge &lt;NGROK_EDGE&gt;\n          ngrok edge\n\n          [env: NGROK_EDGE=]\n\n  -e, --env\n          Display a lot of information about your runtime environment\n\n      --download-only\n          Download model weights only\n\n          [env: DOWNLOAD_ONLY=]\n\n  -h, --help\n          Print help (see a summary with '-h')\n\n  -V, --version\n          Print version\n</code></pre>"},{"location":"reference/metrics/","title":"Metrics","text":"<p>Prometheus-compatible metrics are made available on the default port, on the <code>/metrics</code> endpoint.</p> <p>Below is a list of the metrics that are exposed: | Metric Name                                  | Type      | | -------------------------------------------- | --------- | | <code>lorax_request_count</code>                        | Counter   | | <code>lorax_request_success</code>                      | Counter   | | <code>lorax_request_failure</code>                      | Counter   | | <code>lorax_request_duration</code>                     | Histogram | | <code>lorax_request_queue_duration</code>               | Histogram | | <code>lorax_request_validation_duration</code>          | Histogram | | <code>lorax_request_inference_duration</code>           | Histogram | | <code>lorax_request_mean_time_per_token_duration</code> | Histogram | | <code>lorax_request_generated_tokens</code>             | Histogram | | <code>lorax_request_input_length</code>                 | Histogram |</p> <p>For all histograms, there are metrics that are autogenerated which are the metric name + <code>_sum</code> and <code>_count</code>, which are the sum of all values for that histogram, and the count of all instances of that histogram respectively.</p>"},{"location":"reference/openai_api/","title":"OpenAI Compatible API","text":"<p>LoRAX supports OpenAI Chat Completions v1 compatible endpoints that serve as a drop-in replacement for the OpenAI SDK. It supports multi-turn chat conversations while retaining support for dynamic adapter loading.</p>"},{"location":"reference/openai_api/#chat-completions-v1","title":"Chat Completions v1","text":"<p>Using the existing OpenAI Python SDK, replace the <code>base_url</code> with your LoRAX endpoint with <code>/v1</code> appended. The <code>api_key</code> can be anything, as it is unused.</p> <p>The <code>model</code> parameter can be set to the empty string <code>\"\"</code> to use the base model, or any adapter ID on the HuggingFace hub.</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"EMPTY\",\n    base_url=\"http://127.0.0.1:8080/v1\",\n)\n\nresp = client.chat.completions.create(\n    model=\"alignment-handbook/zephyr-7b-dpo-lora\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n        },\n        {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n    ],\n    max_tokens=100,\n)\nprint(\"Response:\", resp.choices[0].message.content)\n</code></pre>"},{"location":"reference/openai_api/#streaming","title":"Streaming","text":"<p>The streaming API is supported with the <code>stream=True</code> parameter:</p> <pre><code>messages = client.chat.completions.create(\n    model=\"alignment-handbook/zephyr-7b-dpo-lora\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n        },\n        {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n    ],\n    max_tokens=100,\n    stream=True,\n)\n\nfor message in messages:\n    print(message)\n</code></pre>"},{"location":"reference/openai_api/#rest-api","title":"REST API","text":"<p>The REST API can be used directly in addition to the Python SDK:</p> <pre><code>curl http://127.0.0.1:8080/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"model\": \"alignment-handbook/zephyr-7b-dpo-lora\",\n  \"messages\": [\n  {\n      \"role\": \"system\",\n      \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\"\n  },\n  {\n      \"role\": \"user\",\n      \"content\": \"How many helicopters can a human eat in one sitting?\"\n  }\n  ],\n  \"max_tokens\": 100\n}'\n</code></pre>"},{"location":"reference/openai_api/#chat-templates","title":"Chat Templates","text":"<p>Multi-turn chat conversations are supported through HuggingFace chat templates.</p> <p>If the adapter selected with the <code>model</code> parameter has its own tokenizer and chat template, LoRAX will apply the adapter's chat template to the request during inference. If, however, the adapter does not have its own chat template, LoRAX will fallback to using the base model chat template. If this does not exist, an error will be raised, as chat templates are required for multi-turn conversations.</p>"},{"location":"reference/openai_api/#structured-output-json","title":"Structured Output (JSON)","text":"<p>See here for an example.</p>"},{"location":"reference/openai_api/#completions-v1","title":"Completions v1","text":"<p>The legacy completions v1 API can be used as well. This is useful in cases where the model does not have a chat template or you do not wish to interact with the model in a multi-turn conversation.</p> <p>Note, however, that you will need to provide any template boilerplate as part of the <code>prompt</code> as unlike the <code>v1/chat/completions</code> API, it will not be inserted automatically.</p> <p>Note</p> <p>Structured Output (JSON mode) is not supported in the legacy completions API. Please use the Chat Completions API above instead.</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"EMPTY\",\n    base_url=\"http://127.0.0.1:8080/v1\",\n)\n\n# synchronous completions\ncompletion = client.completions.create(\n    model=adapter_id,\n    prompt=prompt,\n)\nprint(\"Completion result:\", completion.choices[0].text)\n\n# streaming completions\ncompletion_stream = client.completions.create(\n    model=adapter_id,\n    prompt=prompt,\n    stream=True,\n)\n\nfor message in completion_stream:\n    print(\"Completion message:\", message)\n</code></pre>"},{"location":"reference/openai_api/#rest-api_1","title":"REST API","text":"<pre><code>curl http://127.0.0.1:8080/v1/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"\",\n\"prompt\": \"Instruct: Write a detailed analogy between mathematics and a lighthouse.\\nOutput:\",\n\"max_tokens\": 100\n}'\n</code></pre>"},{"location":"reference/rest_api/","title":"REST API","text":""},{"location":"reference/python_client/","title":"Python Client","text":"<p>LoRAX Python client provides a convenient way of interfacing with a <code>lorax</code> instance running in your environment.</p>"},{"location":"reference/python_client/#install","title":"Install","text":"<pre><code>pip install lorax-client\n</code></pre>"},{"location":"reference/python_client/#usage","title":"Usage","text":"<pre><code>from lorax import Client\n\nendpoint_url = \"http://127.0.0.1:8080\"\n\nclient = Client(endpoint_url)\ntext = client.generate(\"Why is the sky blue?\", adapter_id=\"some/adapter\").generated_text\nprint(text)\n# ' Rayleigh scattering'\n\n# Token Streaming\ntext = \"\"\nfor response in client.generate_stream(\"Why is the sky blue?\", adapter_id=\"some/adapter\"):\n    if not response.token.special:\n        text += response.token.text\n\nprint(text)\n# ' Rayleigh scattering'\n</code></pre> <p>or with the asynchronous client:</p> <pre><code>from lorax import AsyncClient\n\nendpoint_url = \"http://127.0.0.1:8080\"\n\nclient = AsyncClient(endpoint_url)\nresponse = await client.generate(\"Why is the sky blue?\", adapter_id=\"some/adapter\")\nprint(response.generated_text)\n# ' Rayleigh scattering'\n\n# Token Streaming\ntext = \"\"\nasync for response in client.generate_stream(\"Why is the sky blue?\", adapter_id=\"some/adapter\"):\n    if not response.token.special:\n        text += response.token.text\n\nprint(text)\n# ' Rayleigh scattering'\n</code></pre> <p>See API reference for full details.</p>"},{"location":"reference/python_client/#batch-inference","title":"Batch Inference","text":"<p>In some cases you may have a list of prompts that you wish to process in bulk (\"batch processing\").</p> <p>Rather than process each prompt one at a time, you can take advantage of the <code>AsyncClient</code> and LoRAX's native parallelism to submit your prompts at once and await the results:</p> <pre><code>import asyncio\nimport time\nfrom lorax import AsyncClient\n\n# Batch of prompts to submit\nprompts = [\n    \"The quick brown fox\",\n    \"The rain in Spain\",\n    \"What comes up\",\n]\n\n# Initialize the async client\nendpoint_url = \"http://127.0.0.1:8080\"\nasync_client = AsyncClient(endpoint_url)\n\n# Submit all prompts and do not block on the response\nt0 = time.time()\nfutures = []\nfor prompt in prompts:\n    resp = async_client.generate(prompt, max_new_tokens=64)\n    futures.append(resp)\n\n# Await the completion of all the prompt requests\nresponses = await asyncio.gather(*futures)\n\n# Print responses\n# Responses will always come back in the same order as the original list\nfor resp in responses:\n    print(resp.generated_text)\n\n# Print duration to process all requests in batch\nprint(\"duration (s):\", time.time() - t0)\n</code></pre> <p>Output:</p> <pre><code>duration (s): 2.9093329906463623\n</code></pre> <p>Compare this against the duration of submitting one at a time. You should find that for 3 prompts the duration of async is about 2.5 - 3x faster than serial processing:</p> <pre><code>from lorax import Client\n\nclient = Client(endpoint_url)\n\nt0 = time.time()\nresponses = []\nfor prompt in prompts:\n    resp = client.generate(prompt, max_new_tokens=64)\n    responses.append(resp)\n\nfor resp in responses:\n    print(resp.generated_text)\n\nprint(\"duration (s):\", time.time() - t0)\n</code></pre> <p>Output:</p> <pre><code>duration (s): 8.385080099105835\n</code></pre>"},{"location":"reference/python_client/#predibase-inference-endpoints","title":"Predibase Inference Endpoints","text":"<p>The LoRAX client can also be used to connect to Predibase managed LoRAX endpoints (including Predibase's serverless endpoints).</p> <p>You need only make the following changes to the above examples:</p> <ol> <li>Change the <code>endpoint_url</code> to match the endpoint of your Predibase LLM of choice.</li> <li>Provide your Predibase API token in the <code>headers</code> provided to the client.</li> </ol> <p>Example:</p> <pre><code>from lorax import Client\n\n# You can get your Predibase API token by going to Settings &gt; My Profile &gt; Generate API Token\n# You can get your Predibase Tenant short code by going to Settings &gt; My Profile &gt; Overview &gt; Tenant ID\nendpoint_url = f\"https://serving.app.predibase.com/{predibase_tenant_short_code}/deployments/v2/llms/{llm_deployment_name}\"\nheaders = {\n    \"Authorization\": f\"Bearer {api_token}\"\n}\n\nclient = Client(endpoint_url, headers=headers)\n\n# same as above from here ...\nresponse = client.generate(\"Why is the sky blue?\", adapter_id=f\"{model_repo}/{model_version}\")\n</code></pre> <p>Note that by default Predibase will use its internal model repos as the default <code>adapter_source</code>. To use an adapter from Huggingface:</p> <pre><code>response = client.generate(\"Why is the sky blue?\", adapter_id=\"some/adapter\", adapter_source=\"hub\")\n</code></pre>"},{"location":"reference/python_client/client/","title":"Table of Contents","text":"<ul> <li>lorax.client</li> <li>Client<ul> <li>__init__</li> <li>generate</li> <li>generate_stream</li> </ul> </li> <li>AsyncClient<ul> <li>__init__</li> <li>generate</li> <li>generate_stream</li> </ul> </li> </ul>"},{"location":"reference/python_client/client/#loraxclient","title":"lorax.client","text":""},{"location":"reference/python_client/client/#client-objects","title":"Client Objects","text":"<pre><code>class Client()\n</code></pre> <p>Client to make calls to a LoRAX instance</p> <p>Example:</p> <pre><code>from lorax import Client\n\nclient = Client(\"http://127.0.0.1:8080\")\nclient.generate(\"Why is the sky blue?\", adapter_id=\"some/adapter\").generated_text\n ' Rayleigh scattering'\n\nresult = \"\"\nfor response in client.generate_stream(\"Why is the sky blue?\", adapter_id=\"some/adapter\"):\n    if not response.token.special:\n        result += response.token.text\nresult\n' Rayleigh scattering'\n</code></pre> <p></p>"},{"location":"reference/python_client/client/#__init__","title":"__init__","text":"<pre><code>def __init__(base_url: str,\n             headers: Optional[Dict[str, str]] = None,\n             cookies: Optional[Dict[str, str]] = None,\n             timeout: int = 60)\n</code></pre> <p>Arguments:</p> <ul> <li>base_url (<code>str</code>):   LoRAX instance base url</li> <li>headers (<code>Optional[Dict[str, str]]</code>):   Additional headers</li> <li>cookies (<code>Optional[Dict[str, str]]</code>):   Cookies to include in the requests</li> <li>timeout (<code>int</code>):   Timeout in seconds</li> </ul> <p></p>"},{"location":"reference/python_client/client/#generate","title":"generate","text":"<pre><code>def generate(prompt: str,\n             adapter_id: Optional[str] = None,\n             adapter_source: Optional[str] = None,\n             merged_adapters: Optional[MergedAdapters] = None,\n             api_token: Optional[str] = None,\n             do_sample: bool = False,\n             max_new_tokens: int = 20,\n             best_of: Optional[int] = None,\n             repetition_penalty: Optional[float] = None,\n             return_full_text: bool = False,\n             seed: Optional[int] = None,\n             stop_sequences: Optional[List[str]] = None,\n             temperature: Optional[float] = None,\n             top_k: Optional[int] = None,\n             top_p: Optional[float] = None,\n             truncate: Optional[int] = None,\n             typical_p: Optional[float] = None,\n             watermark: bool = False,\n             response_format: Optional[Union[Dict[str, Any],\n                                             ResponseFormat]] = None,\n             decoder_input_details: bool = False,\n             details: bool = True) -&gt; Response\n</code></pre> <p>Given a prompt, generate the following text</p> <p>Arguments:</p> <ul> <li>prompt (<code>str</code>):   Input text</li> <li>adapter_id (<code>Optional[str]</code>):   Adapter ID to apply to the base model for the request</li> <li>adapter_source (<code>Optional[str]</code>):   Source of the adapter (\"hub\", \"local\", \"s3\", \"pbase\")</li> <li>merged_adapters (<code>Optional[MergedAdapters]</code>):   Merged adapters to apply to the base model for the request</li> <li>api_token (<code>Optional[str]</code>):   API token for accessing private adapters</li> <li>do_sample (<code>bool</code>):   Activate logits sampling</li> <li>max_new_tokens (<code>int</code>):   Maximum number of generated tokens</li> <li>best_of (<code>int</code>):   Generate best_of sequences and return the one if the highest token logprobs</li> <li>repetition_penalty (<code>float</code>):   The parameter for repetition penalty. 1.0 means no penalty. See this   paper for more details.   return_full_text (<code>bool</code>):   Whether to prepend the prompt to the generated text</li> <li>seed (<code>int</code>):   Random sampling seed</li> <li>stop_sequences (<code>List[str]</code>):   Stop generating tokens if a member of <code>stop_sequences</code> is generated</li> <li>temperature (<code>float</code>):   The value used to module the logits distribution.</li> <li>top_k (<code>int</code>):   The number of highest probability vocabulary tokens to keep for top-k-filtering.</li> <li>top_p (<code>float</code>):   If set to &lt; 1, only the smallest set of most probable tokens with probabilities that add up to <code>top_p</code> or   higher are kept for generation.</li> <li>truncate (<code>int</code>):   Truncate inputs tokens to the given size</li> <li>typical_p (<code>float</code>):   Typical Decoding mass   See Typical Decoding for Natural Language Generation for more information</li> <li>watermark (<code>bool</code>):   Watermarking with A Watermark for Large Language Models</li> <li>response_format (<code>Optional[Union[Dict[str, Any], ResponseFormat]]</code>):   Optional specification of a format to impose upon the generated text, e.g.,:         <pre><code>{\n    \"type\": \"json_object\",\n    \"schema\": {\n        \"type\": \"string\",\n        \"title\": \"response\"\n    }\n}\n</code></pre></li> <li>decoder_input_details (<code>bool</code>):   Return the decoder input token logprobs and ids</li> <li>details (<code>bool</code>):   Return the token logprobs and ids for generated tokens</li> </ul> <p>Returns:</p> <ul> <li><code>Response</code> - generated response</li> </ul> <p></p>"},{"location":"reference/python_client/client/#generate_stream","title":"generate_stream","text":"<pre><code>def generate_stream(prompt: str,\n                    adapter_id: Optional[str] = None,\n                    adapter_source: Optional[str] = None,\n                    merged_adapters: Optional[MergedAdapters] = None,\n                    api_token: Optional[str] = None,\n                    do_sample: bool = False,\n                    max_new_tokens: int = 20,\n                    repetition_penalty: Optional[float] = None,\n                    return_full_text: bool = False,\n                    seed: Optional[int] = None,\n                    stop_sequences: Optional[List[str]] = None,\n                    temperature: Optional[float] = None,\n                    top_k: Optional[int] = None,\n                    top_p: Optional[float] = None,\n                    truncate: Optional[int] = None,\n                    typical_p: Optional[float] = None,\n                    watermark: bool = False,\n                    response_format: Optional[Union[Dict[str, Any],\n                                                    ResponseFormat]] = None,\n                    details: bool = True) -&gt; Iterator[StreamResponse]\n</code></pre> <p>Given a prompt, generate the following stream of tokens</p> <p>Arguments:</p> <ul> <li>prompt (<code>str</code>):   Input text</li> <li>adapter_id (<code>Optional[str]</code>):   Adapter ID to apply to the base model for the request</li> <li>adapter_source (<code>Optional[str]</code>):   Source of the adapter (hub, local, s3)</li> <li>merged_adapters (<code>Optional[MergedAdapters]</code>):   Merged adapters to apply to the base model for the request</li> <li>api_token (<code>Optional[str]</code>):   API token for accessing private adapters</li> <li>do_sample (<code>bool</code>):   Activate logits sampling</li> <li>max_new_tokens (<code>int</code>):   Maximum number of generated tokens</li> <li>repetition_penalty (<code>float</code>):   The parameter for repetition penalty. 1.0 means no penalty. See this   paper for more details.   return_full_text (<code>bool</code>):   Whether to prepend the prompt to the generated text</li> <li>seed (<code>int</code>):   Random sampling seed</li> <li>stop_sequences (<code>List[str]</code>):   Stop generating tokens if a member of <code>stop_sequences</code> is generated</li> <li>temperature (<code>float</code>):   The value used to module the logits distribution.</li> <li>top_k (<code>int</code>):   The number of highest probability vocabulary tokens to keep for top-k-filtering.</li> <li>top_p (<code>float</code>):   If set to &lt; 1, only the smallest set of most probable tokens with probabilities that add up to <code>top_p</code> or   higher are kept for generation.</li> <li>truncate (<code>int</code>):   Truncate inputs tokens to the given size</li> <li>typical_p (<code>float</code>):   Typical Decoding mass   See Typical Decoding for Natural Language Generation for more information</li> <li>watermark (<code>bool</code>):   Watermarking with A Watermark for Large Language Models   response_format (<code>Optional[Union[Dict[str, Any], ResponseFormat]]</code>):   Optional specification of a format to impose upon the generated text, e.g.,:         <pre><code>{\n    \"type\": \"json_object\",\n    \"schema\": {\n        \"type\": \"string\",\n        \"title\": \"response\"\n    }\n}\n</code></pre></li> <li>details (<code>bool</code>):   Return the token logprobs and ids for generated tokens</li> </ul> <p>Returns:</p> <ul> <li><code>Iterator[StreamResponse]</code> - stream of generated tokens</li> </ul> <p></p>"},{"location":"reference/python_client/client/#asyncclient-objects","title":"AsyncClient Objects","text":"<pre><code>class AsyncClient()\n</code></pre> <p>Asynchronous Client to make calls to a LoRAX instance</p> <p>Example:</p> <pre><code>from lorax import AsyncClient\n\nclient = AsyncClient(\"https://api-inference.huggingface.co/models/bigscience/bloomz\")\nresponse = await client.generate(\"Why is the sky blue?\", adapter_id=\"some/adapter\")\nresponse.generated_text\n' Rayleigh scattering'\n\nresult = \"\"\nasync for response in client.generate_stream(\"Why is the sky blue?\", adapter_id=\"some/adapter\"):\n    if not response.token.special:\n        result += response.token.text\nresult\n' Rayleigh scattering'\n</code></pre> <p></p>"},{"location":"reference/python_client/client/#__init___1","title":"__init__","text":"<pre><code>def __init__(base_url: str,\n             headers: Optional[Dict[str, str]] = None,\n             cookies: Optional[Dict[str, str]] = None,\n             timeout: int = 60)\n</code></pre> <p>Arguments:</p> <ul> <li>base_url (<code>str</code>):   LoRAX instance base url</li> <li>headers (<code>Optional[Dict[str, str]]</code>):   Additional headers</li> <li>cookies (<code>Optional[Dict[str, str]]</code>):   Cookies to include in the requests</li> <li>timeout (<code>int</code>):   Timeout in seconds</li> </ul> <p></p>"},{"location":"reference/python_client/client/#generate_1","title":"generate","text":"<pre><code>async def generate(prompt: str,\n                   adapter_id: Optional[str] = None,\n                   adapter_source: Optional[str] = None,\n                   merged_adapters: Optional[MergedAdapters] = None,\n                   api_token: Optional[str] = None,\n                   do_sample: bool = False,\n                   max_new_tokens: int = 20,\n                   best_of: Optional[int] = None,\n                   repetition_penalty: Optional[float] = None,\n                   return_full_text: bool = False,\n                   seed: Optional[int] = None,\n                   stop_sequences: Optional[List[str]] = None,\n                   temperature: Optional[float] = None,\n                   top_k: Optional[int] = None,\n                   top_p: Optional[float] = None,\n                   truncate: Optional[int] = None,\n                   typical_p: Optional[float] = None,\n                   watermark: bool = False,\n                   response_format: Optional[Union[Dict[str, Any],\n                                                   ResponseFormat]] = None,\n                   decoder_input_details: bool = False,\n                   details: bool = True) -&gt; Response\n</code></pre> <p>Given a prompt, generate the following text asynchronously</p> <p>Arguments:</p> <ul> <li>prompt (<code>str</code>):   Input text</li> <li>adapter_id (<code>Optional[str]</code>):   Adapter ID to apply to the base model for the request</li> <li>adapter_source (<code>Optional[str]</code>):   Source of the adapter (hub, local, s3)</li> <li>merged_adapters (<code>Optional[MergedAdapters]</code>):   Merged adapters to apply to the base model for the request</li> <li>api_token (<code>Optional[str]</code>):   API token for accessing private adapters</li> <li>do_sample (<code>bool</code>):   Activate logits sampling</li> <li>max_new_tokens (<code>int</code>):   Maximum number of generated tokens</li> <li>best_of (<code>int</code>):   Generate best_of sequences and return the one if the highest token logprobs   repetition_penalty (<code>float</code>):   The parameter for repetition penalty. 1.0 means no penalty. See this   paper for more details.   return_full_text (<code>bool</code>):   Whether to prepend the prompt to the generated text</li> <li>seed (<code>int</code>):   Random sampling seed</li> <li>stop_sequences (<code>List[str]</code>):   Stop generating tokens if a member of <code>stop_sequences</code> is generated</li> <li>temperature (<code>float</code>):   The value used to module the logits distribution.</li> <li>top_k (<code>int</code>):   The number of highest probability vocabulary tokens to keep for top-k-filtering.</li> <li>top_p (<code>float</code>):   If set to &lt; 1, only the smallest set of most probable tokens with probabilities that add up to <code>top_p</code> or   higher are kept for generation.</li> <li>truncate (<code>int</code>):   Truncate inputs tokens to the given size</li> <li>typical_p (<code>float</code>):   Typical Decoding mass   See Typical Decoding for Natural Language Generation for more information</li> <li>watermark (<code>bool</code>):   Watermarking with A Watermark for Large Language Models</li> <li>response_format (<code>Optional[Union[Dict[str, Any], ResponseFormat]]</code>):   Optional specification of a format to impose upon the generated text, e.g.,:         <pre><code>{\n    \"type\": \"json_object\",\n    \"schema\": {\n        \"type\": \"string\",\n        \"title\": \"response\"\n    }\n}\n</code></pre></li> <li>decoder_input_details (<code>bool</code>):   Return the decoder input token logprobs and ids</li> <li>details (<code>bool</code>):   Return the token logprobs and ids for generated tokens</li> </ul> <p>Returns:</p> <ul> <li><code>Response</code> - generated response</li> </ul> <p></p>"},{"location":"reference/python_client/client/#generate_stream_1","title":"generate_stream","text":"<pre><code>async def generate_stream(\n        prompt: str,\n        adapter_id: Optional[str] = None,\n        adapter_source: Optional[str] = None,\n        merged_adapters: Optional[MergedAdapters] = None,\n        api_token: Optional[str] = None,\n        do_sample: bool = False,\n        max_new_tokens: int = 20,\n        repetition_penalty: Optional[float] = None,\n        return_full_text: bool = False,\n        seed: Optional[int] = None,\n        stop_sequences: Optional[List[str]] = None,\n        temperature: Optional[float] = None,\n        top_k: Optional[int] = None,\n        top_p: Optional[float] = None,\n        truncate: Optional[int] = None,\n        typical_p: Optional[float] = None,\n        watermark: bool = False,\n        response_format: Optional[Union[Dict[str, Any],\n                                        ResponseFormat]] = None,\n        details: bool = True) -&gt; AsyncIterator[StreamResponse]\n</code></pre> <p>Given a prompt, generate the following stream of tokens asynchronously</p> <p>Arguments:</p> <ul> <li>prompt (<code>str</code>):   Input text</li> <li>adapter_id (<code>Optional[str]</code>):   Adapter ID to apply to the base model for the request</li> <li>adapter_source (<code>Optional[str]</code>):   Source of the adapter (hub, local, s3)</li> <li>merged_adapters (<code>Optional[MergedAdapters]</code>):   Merged adapters to apply to the base model for the request</li> <li>api_token (<code>Optional[str]</code>):   API token for accessing private adapters</li> <li>do_sample (<code>bool</code>):   Activate logits sampling</li> <li>max_new_tokens (<code>int</code>):   Maximum number of generated tokens</li> <li>repetition_penalty (<code>float</code>):   The parameter for repetition penalty. 1.0 means no penalty. See this   paper for more details.   return_full_text (<code>bool</code>):   Whether to prepend the prompt to the generated text</li> <li>seed (<code>int</code>):   Random sampling seed</li> <li>stop_sequences (<code>List[str]</code>):   Stop generating tokens if a member of <code>stop_sequences</code> is generated</li> <li>temperature (<code>float</code>):   The value used to module the logits distribution.</li> <li>top_k (<code>int</code>):   The number of highest probability vocabulary tokens to keep for top-k-filtering.</li> <li>top_p (<code>float</code>):   If set to &lt; 1, only the smallest set of most probable tokens with probabilities that add up to <code>top_p</code> or   higher are kept for generation.</li> <li>truncate (<code>int</code>):   Truncate inputs tokens to the given size</li> <li>typical_p (<code>float</code>):   Typical Decoding mass   See Typical Decoding for Natural Language Generation for more information</li> <li>watermark (<code>bool</code>):   Watermarking with A Watermark for Large Language Models</li> <li>response_format (<code>Optional[Union[Dict[str, Any], ResponseFormat]]</code>):   Optional specification of a format to impose upon the generated text, e.g.,:         <pre><code>{\n    \"type\": \"json_object\",\n    \"schema\": {\n        \"type\": \"string\",\n        \"title\": \"response\"\n    }\n}\n</code></pre></li> <li>details (<code>bool</code>):   Return the token logprobs and ids for generated tokens</li> </ul> <p>Returns:</p> <ul> <li><code>AsyncIterator[StreamResponse]</code> - stream of generated tokens</li> </ul>"}]}