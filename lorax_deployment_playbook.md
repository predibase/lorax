# ğŸš€ LoRAX Deployment Playbook

Welcome to the **LoRAX Deployment Playbook**! This guide is designed for **first-time operators** setting up a **LoRAX server** on a fresh **Ubuntu 22.04** GPU host with **sudo** access. We'll walk you through each step, explain *why* it matters, and provide quick fixes for common issues. Let's get your **LoRAX server** up and running! ğŸ‰

> **Goal:** Deploy a working **LoRAX server** with a chosen model, understand the process, and troubleshoot issues fast.

---

## ğŸ“‹ Overview

To deploy **LoRAX**, you need these components in order:

1. **GPU Driver** â€“ Verify `nvidia-smi` works on the host.
2. **Docker Engine** â€“ Ensure the user is in the `docker` group.
3. **NVIDIA Container Runtime** â€“ Make GPUs accessible inside containers.
4. **LoRAX Container** â€“ Pull or build the container image.
5. **Model Files** â€“ Download or cache model files.
6. **API** â€“ Confirm the server is listening and passes a basic inference test.

> **Quick Sanity Check:** Stop at the first failure in this sequence:
> - **A.** Run `nvidia-smi` on the host.
> - **B.** Test GPU access in a container: `docker run --rm --gpus all nvidia/cuda:12.4.0-base-ubuntu22.04 nvidia-smi`.
> - **C.** Launch **LoRAX** with `MODEL_ID=gpt2`.
> - **D.** Test the API with `curl`.
> - **E.** Scale up to a larger model.

---

## Phase 1: Host Setup

### 1. Verify NVIDIA Driver âœ…

Ensure your **NVIDIA driver** is working correctly.

```bash
nvidia-smi
```

**Success:** Displays a table with the driver version and GPU details.  
**Common Failures:**
- *`command not found`* â†’ Driver not installed or PATH issue.
- *â€œNVIDIA-SMI has failedâ€* â†’ Kernel module mismatch or Secure Boot blocking.

> **Fix:** Reinstall the NVIDIA driver or disable/enroll MOK for Secure Boot.

---

### 2. Install Docker ğŸ³

Set up **Docker** to run containers on **Ubuntu 22.04**.

```bash
sudo apt update
sudo apt install -y ca-certificates curl
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc
echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt update
sudo apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
```

**What This Does:**
- Updates package metadata.
- Installs tools for HTTPS repositories.
- Sets up Dockerâ€™s GPG key and repository.
- Installs **Docker Engine**, CLI, and plugins.

**Success:** Run `docker --version` and `systemctl status docker` (should show *active (running)*).  
**Common Failures:**
- GPG/repo errors (â€œNO_PUBKEYâ€, â€œUnsignedâ€) â†’ Key issue; redo key setup.
- Architecture mismatch on non-x86 hosts.

> **Fix:** Re-run key download steps and `apt update`.

---

### 3. Install NVIDIA Container Toolkit ğŸ”§

Enable GPU access inside **Docker containers**.

```bash
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/ubuntu22.04/libnvidia-container.list | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#' | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list > /dev/null
sudo apt update
sudo apt install -y nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
```

**What This Does:**
- Adds the NVIDIA Container Toolkit repository.
- Installs the toolkit and configures Docker to use NVIDIA GPUs.

**Success:** Check `/etc/docker/daemon.json` for `runtimes.nvidia`. Test with a CUDA container (Step 5).  
**Common Failures:**
- `nvidia-ctk: command not found` â†’ Installation failed; redo apt steps.
- â€œCould not select device driverâ€ â†’ Runtime misconfigured; re-run configure and restart.

> **Fix:** Re-run the toolkit installation and configuration steps.

---

### 4. Add User to Docker Group ğŸ‘¤

Allow running **Docker** commands without `sudo`.

```bash
sudo usermod -aG docker $USER
newgrp docker
```

**Success:** `groups` shows `docker`; `docker ps` works without `sudo`.  
**Common Failure:** Commands still require `sudo` â†’ Log out and back in.

> **Tip:** Log out and log back in to apply group changes.

---

### 5. Verify GPU in Container ğŸ–¥ï¸

Confirm GPUs are accessible inside a **Docker container**.

```bash
docker run --rm --gpus all nvidia/cuda:12.4.0-base-ubuntu22.04 nvidia-smi
```

**Success:** Displays a table similar to `nvidia-smi` on the host.  
**Common Failures:**
- â€œUnknown runtime specified nvidiaâ€ â†’ Toolkit setup incomplete (redo Step 3).
- â€œCUDA driver version insufficientâ€ â†’ Host driver outdated; update it.
- â€œCould not select device driverâ€ â†’ Runtime misconfigured; redo Step 3.

> **Fix:** Revisit NVIDIA Container Toolkit setup or update the host driver.

---

## Phase 2: Deploy LoRAX

Choose one deployment path:
- **(A) Pre-built Image** â€“ Fastest option, recommended for most users.
- **(B) Build from Source** â€“ Only for custom changes or unreleased patches.

### Option A: Pre-built Image ğŸ‰

#### 1. Pull the LoRAX Image

```bash
docker pull ghcr.io/predibase/lorax:main
```


**Success:** Image downloads successfully.  
**Common Failure:** Network timeout â†’ Retry or check connectivity.

> **Tip:** This is a public image, so no authentication issues are expected.

---

#### 2. Choose Your Model ğŸ“Š

Start with **`gpt2`** for a quick test (itâ€™s small and fast). Larger models require careful **VRAM** planning to avoid `CUDA out of memory` errors.

| **Model** | **Params** | **VRAM (FP16/BF16)** | **Notes** |
|-----------|------------|-----------------------|-----------|
| `gpt2` | 0.1B | ~0.5 GB | Perfect for testing; fits any GPU. |
| `bigcode/starcoder2-3b` | 3B | ~6â€“7 GB | Works on 8 GB VRAM GPUs. |
| `mistralai/Mistral-7B-Instruct-v0.3` | 7B | ~14â€“15 GB | Needs 16â€“24 GB VRAM. |
| `meta-llama/Meta-Llama-3-8B-Instruct` | 8B | ~16 GB | Tight on 16 GB; better with 24 GB. |
| `TheBloke/Mistral-7B-Instruct-v0.3-GPTQ` | 7B (4-bit) | ~8â€“10 GB | Quantized; fits 12â€“16 GB VRAM. |
| `meta-llama/Meta-Llama-3-13B-Instruct` | 13B | ~26 GB | Requires 24â€“26 GB VRAM. |
| `meta-llama/Meta-Llama-3-70B-Instruct` | 70B | 135â€“140 GB | Needs multi-GPU or heavy quantization. |

> **VRAM Tips:**
> - Keep **10â€“15% VRAM free** for KV cache and overhead.
> - **6â€“8 GB GPUs**: Stick to `gpt2` or quantized 7B models.
> - **12â€“16 GB GPUs**: Comfortable for 7B; tight for 8B.
> - **24 GB+ GPUs**: Suitable for 13B or multi-instance setups.

---

#### 3. Run the LoRAX Container

```bash
MODEL_ID="gpt2"
SHARDED_MODEL="false"
PORT=80

docker run --rm 

--name lorax 

--gpus all 

-e HUGGING_FACE_HUB_TOKEN="$HUGGING_FACE_HUB_TOKEN" 

-e TRANSFORMERS_CACHE=/data 

-v "$HOME/lorax_model_cache":/data 

-v "$HOME/lorax_outlines_cache":/root/.cache/outlines 

--user "$(id -u):$(id -g)" 

-p ${PORT}:80 

ghcr.io/predibase/lorax:main 

--model-id "$MODEL_ID" 

--sharded "$SHARDED_MODEL"
```


**What This Does:**
- Starts the **LoRAX container** with GPU access.
- Mounts model cache to persist downloads.
- Maps port **80** (container) to your chosen **host port**.
- Loads the specified **model** (start with `gpt2`).

**Success:** Logs show model download/cache hit and â€œModel loadedâ€; health endpoint responds.  
**Common Failures:**
- Stalled download â†’ Network or Hugging Face rate limits.
- `CUDA out of memory` â†’ Model too large for GPU VRAM.

> **Fix for Stalled Downloads:**
> 1. Visit the modelâ€™s Hugging Face page (e.g., `https://huggingface.co/<model_id>/tree/main`).
> 2. Note the commit hash from the URL or â€œFiles and Versions.â€
> 3. Create the cache path: `$HOME/lorax_model_cache/<model_id>/snapshots/<commit_hash>/`.
> 4. Download all model files (config, tokenizer, `.safetensors`, etc.) to that directory.
> 5. Re-run the container; it should use the cached files.

---

### Option B: Build from Source ğŸ› ï¸

Use this if you need custom changes or unreleased patches.

#### 1. Clone the Repository

```bash
git clone https://github.com/predibase/lorax.git
cd lorax
```

#### 2. Initialize Submodules (if needed)

```bash
git submodule update --init --recursive
```


#### 3. Build the Image

```bash
docker build -t my-lorax-server -f Dockerfile .
```


**Common Failures:**
- Build stalls â†’ Add `--network=host` to the build command.
- Version conflicts â†’ Adjust base image or dependencies.

#### 4. Run the Container

Use the same `docker run` command as in Option A, replacing `ghcr.io/predibase/lorax:main` with `my-lorax-server`.

**Common Failures:**
- â€œExec format errorâ€ â†’ Image built for wrong architecture.
- Immediate exit â†’ Library mismatch; rebuild with compatible CUDA base.

---

## Phase 3: Test the API

Once logs show the server is ready, test the **LoRAX API**.

```bash
curl http://localhost:80/
```


**Example Inference:**

```bash
curl -X POST http://localhost:80/generate 

-H 'Content-Type: application/json' 

-d '{"prompt":"Hello","max_tokens":32}'
```


**Success:** Returns JSON with generated text.  
**Common Failures:**
- Connection refused â†’ Container not running or wrong port (`docker ps`).
- 404 â†’ Wrong endpoint; check root docs.
- 500 â†’ Model not loaded or OOM (`docker logs lorax`).

> **Fix:** Check logs with `docker logs lorax` and verify port mapping.

---

## Phase 4: Performance & Scaling Tips

- **Concurrency:** Increase only after single-request stability (KV cache can cause OOM).
- **Tuning Options:** Adjust `--max-concurrent-requests`, batching, or tensor parallelization (if supported).
- **Monitor GPUs:**

```bash
watch -n1 nvidia-smi
```


---

## Troubleshooting Guide

**Format:** [Stage] Symptom â†’ Cause â†’ Fix

- **[Host]** `nvidia-smi` fails â†’ Driver issue â†’ Check `dmesg | grep -i nvidia | tail -n5`; reinstall driver or fix Secure Boot.
- **[Container]** â€œCould not select device driverâ€ â†’ Runtime misconfigured â†’ Verify `/etc/docker/daemon.json`; redo toolkit setup.
- **[Docker]** Cache permission denied â†’ Root-owned files â†’ Run `sudo chown -R $(id -u):$(id -g) $HOME/lorax_model_cache`.
- **[Model Load]** CUDA OOM â†’ Model too large â†’ Check `nvidia-smi`; use smaller/quantized model.
- **[Model Load]** Download stalls â†’ Network issue â†’ Use manual download workaround.
- **[Model Load]** `RuntimeError: weight not found` â†’ Quantized model incompatibility â†’ Try FP16 or a different quantized model.
- **[API]** 404 on generate â†’ Wrong route â†’ Check `curl http://localhost:80/`; adjust client.
- **[API]** 500 error â†’ OOM or bad params â†’ Check `docker logs --tail 100 lorax | grep -i error`; reduce `max_tokens`.
- **[Performance]** Slow first call â†’ Warmup overhead â†’ Send a short warmup prompt.
- **[Performance]** Low GPU usage (<30%) â†’ Small batches â†’ Enable batching or increase concurrency.
- **[Stability]** Exit code 137 â†’ Host OOM â†’ Check `dmesg | tail`; reduce model size.

---

## ğŸ§  Decision Matrix

| **Situation** | **Action** |
|---------------|------------|
| `nvidia-smi` broken | Fix driver first. |
| Container `nvidia-smi` fails | Fix NVIDIA runtime config. |
| `gpt2` fails to load | Check environment/image. |
| `gpt2` works, larger model fails | Address VRAM/quantization issues. |
| API fails | Check routes, params, or logs. |
| API slow | Optimize concurrency or use smaller model. |

---

## ğŸ§¹ Cleanup & Reset

```bash
docker stop lorax
docker system prune -f
rm -rf $HOME/lorax_model_cache/*
sudo chown -R $(id -u):$(id -g) $HOME/lorax_model_cache
```


---

## ğŸ“œ Quick Command Recap

```bash
nvidia-smi
docker run --rm --gpus all nvidia/cuda:12.4.0-base-ubuntu22.04 nvidia-smi
docker pull ghcr.io/predibase/lorax:main
MODEL_ID="gpt2"; docker run --rm --gpus all -v "$HOME/lorax_model_cache":/data -p 80:80 ghcr.io/predibase/lorax:main --model-id "$MODEL_ID" --sharded false
curl http://localhost:80/
```


---

## ğŸŒŸ Next Steps

- **Monitoring:** Add logging/metrics with Prometheus or parse stdout.
- **Security:** Set up a reverse proxy (nginx/traefik) with TLS for public access.
- **Automation:** Create health/warmup scripts (e.g., systemd or Docker Compose).
- **Reliability:** Add watchdog with `Restart=on-failure` (systemd or Docker policies).

---

**Happy Deploying!** ğŸ‰

